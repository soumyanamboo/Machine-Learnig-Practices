{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Outline\n",
        "\n",
        "In this colab, we study how to handle **large-scale datasets** in sklearn.  \n",
        "\n",
        "* In this course, so far we were able to load **entire data in memory** and were able to train and make inferences on all the data at once.  \n",
        "\n",
        "* The large scale data sets may not fit in memory and we need to devise strategies to handle it in the context of training and prediction use cases. \n",
        "\n",
        "In this colab, we will discuss the following topics:\n",
        "> - Overview of handling large-scale data\n",
        "- Incremental preprocessing and learning.\n",
        "  -  `fit()` vs. `partial_fit()`: `partial_fit` is our friend in this case.\n",
        "- Combining preprocessing and incremental learning."
      ],
      "metadata": {
        "id": "aoH0jmZD-6m6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Large-scale Machine Learning**"
      ],
      "metadata": {
        "id": "N5i-LjWn3Ws0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Large-scale Machine Learning differs from traditional machine learning in the sense that it involves processing large amount of data in terms of its **size** or **number of samples**, **features** or **classes**.\n",
        "\n",
        "There were many exciting developments in efficient large scale learning on many real world use cases in the last decade."
      ],
      "metadata": {
        "id": "vv6jdhLh2QiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although scikit-learn is optimized for **smaller data**, it does offer a decent set of **feature preprocessing** and **learning algorithms** such as classification, regression and clustering for large scale data . \n",
        "\n",
        "Scikit-learn handles large data through `partial_fit()` method instead of using the usual `fit()` method. \n",
        "> The idea is to process data in **batches** and **update** the model parameters for each batch.  This way of learning is referred to as '**Incremental (or out-of-core) learning**'."
      ],
      "metadata": {
        "id": "Urd-kNUf35op"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Incremental Learning\n",
        "\n",
        "Increamental learning may be required in the following two scenarios:\n",
        "\n",
        "* For **out-of-memory (large) datasets**, where it’s not possible to **load the entire data into the RAM** at once, one can load the data in chunks and fit the training model for each chunk of data.\n",
        "\n",
        "* For machine learning tasks where a new batch of data comes with time, re-training the model with the previous and new batch of data is a computationally expensive process. \n",
        "> Instead of re-training the model with the entire set of data, one can employ an incremental learning approach, where the model parameters are updated with the new batch of data.\n"
      ],
      "metadata": {
        "id": "z4tUmPfflGLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Incremental Learning in `sklearn`\n",
        "\n",
        "To perform incremental learning, Scikit-learn implements **`partial_fit`** method that helps in training an out-of-memory dataset. In other words, it has the ability to learn incrementally from a batch of instances.\n",
        "\n",
        "In this colab, we will see an example of how to read, process, and train on such a large dataset that can't be loaded in memory entirely. \n",
        "\n",
        "This method is expected to be called several times consecutively on different chunks of a dataset so as to implement out-of-core (online) learning. This function has some performance overhead, so it’s recommended to call it on a considerable large batch of data (that fits into the memory), to overcome the limitation of overhead."
      ],
      "metadata": {
        "id": "gaGFocEDnA9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### partial_fit() attributes:\n",
        "\n",
        "`partial_fit(X, y, [classes], [sample_weight])`\n",
        "\n",
        "where,\n",
        "\n",
        "* `X` : array of shape (n_samples, n_features) where n_samples is the number of samples and n_features is the number of features.\n",
        "\n",
        "* `y` : array of shape (n_samples,) of target values.\n",
        "\n",
        "* `classes` : array of shape (n_classes,) containing a list of all the classes that can possibly appear in the y vector.\n",
        "\n",
        "Must be provided at the first call to partial_fit, can be omitted in subsequent calls.\n",
        "\n",
        "* `sample_weight` : (optional) array of shape (n_samples,) containing weights applied to individual samples (1. for unweighted).\n",
        "\n",
        "Returns: object (self)"
      ],
      "metadata": {
        "id": "zo3oOSHM52_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For classification tasks, we have to pass the list of possible target class labels in `classes` parameter to cope-up with the unseen target classes in the 1st batch of the data.\n",
        "\n",
        "The following estimators implement `partial_fit` method:\n",
        "* **Classification:** \n",
        "  * `MultinomialNB`\n",
        "  * `BernoulliNB`\n",
        "  * `SGDClassifier`\n",
        "  * `Perceptron`\n",
        "\n",
        "* **Regression:** \n",
        "  * `SGDRegressor`\n",
        "\n",
        "* **Clustering:** \n",
        "  * `MiniBatchKMeans`\n"
      ],
      "metadata": {
        "id": "0N4T09hEoYQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`SGDRegressor` and `SGDClassifier` are commonly used for handling large data. \n",
        "\n",
        "The problem with standard regression/classification implementations such as batch gradient descent, support vector machines (SVMs), random forests etc is that because of the need to load all the data into memory at once, they can not be used in scenarios where we do not have sufficient memory. SGD, however, can deal with large data sets effectively by breaking up the data into chunks and processing them sequentially. The fact that we only need to load one chunk into memory at a time makes it useful for large-scale data as well as cases where we get streams of data at intervals. "
      ],
      "metadata": {
        "id": "gnj-lIf_Ry_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**fit() versus partial_fit()**"
      ],
      "metadata": {
        "id": "UBDGMYiCyeWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we show the use of `partial_fit()` along with `SGDClassifier` on a sample data.\n",
        "\n",
        "For illustration, we first use traditional `fit()` method and then use `partial_fit()' on the same data. "
      ],
      "metadata": {
        "id": "EoR4-kSZAgxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "J_EgPxUAaQ0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Traditional Approach (using `fit()`)"
      ],
      "metadata": {
        "id": "2xaSRms-bTDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sample dataset\n",
        "\n",
        "We will use a synthetic classification dataset for demonstration. \n",
        "\n",
        "Let us have 50000 samples with 10 features in the feature matrix. Further, lets have 3 classes in the target label, each class having a single cluster."
      ],
      "metadata": {
        "id": "hPbFOqDrA_lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = make_classification(n_samples=50000, n_features=10, \n",
        "                           n_classes=3, \n",
        "                           n_clusters_per_class=1)\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.15)\n"
      ],
      "metadata": {
        "id": "WZ39P1HDaT1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will make use of `SGDClassifier` to learn the classification model."
      ],
      "metadata": {
        "id": "kqyewfEKORwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf1 = SGDClassifier(max_iter=1000, tol=0.01)\n"
      ],
      "metadata": {
        "id": "GeqBnlSGadHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use traditional `fit()` method to train our model."
      ],
      "metadata": {
        "id": "_EP7CfcobywU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf1.fit(xtrain, ytrain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoC1b1aVbxOm",
        "outputId": "1120277b-2ac8-4f5b-ce7d-c3e91d08e1b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(tol=0.01)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's obtain the training and test scores on the trained model."
      ],
      "metadata": {
        "id": "jpk1NHHxOXhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_score = clf1.score(xtrain, ytrain)\n",
        "print(\"Training score: \", train_score) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAeVY_fbaf7P",
        "outputId": "a8ca299b-1aca-4706-9f9c-ce79de062f8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training score:  0.8740470588235294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_score = clf1.score(xtest, ytest)\n",
        "print(\"Test score: \", test_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_R9jn0Wa_tm",
        "outputId": "7209a267-9696-4349-8bf4-6b7c27dc528b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test score:  0.8718666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We obtain the confusion matrix and classification report for evaluating the classifier."
      ],
      "metadata": {
        "id": "_3r1-785OnOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ypred = clf1.predict(xtest)\n",
        "\n",
        "cm = confusion_matrix(ytest, ypred)\n",
        "print(cm) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibp-Am5Tansz",
        "outputId": "871c5ecf-0ba0-42be-ebe5-ef86d55abf9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2363   74  109]\n",
            " [ 250 1842  408]\n",
            " [ 101   19 2334]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use `classification_report` API for obtaining important evaluation metrics for all three classes."
      ],
      "metadata": {
        "id": "A4biNqb_OwDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cr = classification_report(ytest, ypred)\n",
        "print(cr) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-QFDYxwawu0",
        "outputId": "49df9216-259e-4fdd-d58f-ada4656088ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.93      0.90      2546\n",
            "           1       0.95      0.74      0.83      2500\n",
            "           2       0.82      0.95      0.88      2454\n",
            "\n",
            "    accuracy                           0.87      7500\n",
            "   macro avg       0.88      0.87      0.87      7500\n",
            "weighted avg       0.88      0.87      0.87      7500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Incremental approach (using partial_fit())\n",
        "\n",
        "We will now assume that the data can not be kept completely in the main memory and hence, will load chunks of data and fit using `partial_fit()`."
      ],
      "metadata": {
        "id": "rn5L-7hvbO8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHhyq_DOeeC6",
        "outputId": "55ede71a-6e9e-495e-ac80-ea82ff51167a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.68757687, -0.64550522,  0.62225691, -0.47085352,  0.40740261,\n",
              "         1.26881267,  0.88761506,  0.65488746, -0.64590846,  0.11256402],\n",
              "       [-0.89396789, -0.80955777,  0.37497888,  1.3207705 ,  0.27718403,\n",
              "        -2.25014512,  1.52162745,  1.33110039,  1.73783113,  1.55400338],\n",
              "       [ 0.0255818 ,  0.62806094,  0.73782478,  0.2250157 ,  0.56208794,\n",
              "         0.60477614, -0.43315007,  1.37549026,  0.2692454 , -1.61447111],\n",
              "       [-1.52689091,  0.37674956,  1.97143845,  0.56636717, -0.99702095,\n",
              "        -1.40439757, -1.38714459, -1.91884614,  0.80334504,  0.87205605],\n",
              "       [ 2.57706481, -1.71781642,  0.86873806,  0.97826578,  0.50814283,\n",
              "        -1.79168631,  0.23275466,  1.66158004,  1.27139602,  0.1363464 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ytrain[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMBV0AfKejvF",
        "outputId": "f0245540-8a8e-457b-d60a-b79408f3e075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 2, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In order to load data chunk by chunk, we will first store the given (training) data in a csv file. (This is just for demonstration purpose. In a real scenario, the large dataset might already be in the form of say, a csv, which we will be reading in multiple iterations.)"
      ],
      "metadata": {
        "id": "gKXDg16IBwFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "-JnqFcXtenvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.concatenate((xtrain, ytrain[:, np.newaxis]), axis=1)"
      ],
      "metadata": {
        "id": "8Z6hsSzLepsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVww861BhHRc",
        "outputId": "3a124cdc-be75-4de8-a0c6-a55ab024659b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.68757687, -0.64550522,  0.62225691, -0.47085352,  0.40740261,\n",
              "         1.26881267,  0.88761506,  0.65488746, -0.64590846,  0.11256402,\n",
              "         1.        ],\n",
              "       [-0.89396789, -0.80955777,  0.37497888,  1.3207705 ,  0.27718403,\n",
              "        -2.25014512,  1.52162745,  1.33110039,  1.73783113,  1.55400338,\n",
              "         0.        ],\n",
              "       [ 0.0255818 ,  0.62806094,  0.73782478,  0.2250157 ,  0.56208794,\n",
              "         0.60477614, -0.43315007,  1.37549026,  0.2692454 , -1.61447111,\n",
              "         0.        ],\n",
              "       [-1.52689091,  0.37674956,  1.97143845,  0.56636717, -0.99702095,\n",
              "        -1.40439757, -1.38714459, -1.91884614,  0.80334504,  0.87205605,\n",
              "         2.        ],\n",
              "       [ 2.57706481, -1.71781642,  0.86873806,  0.97826578,  0.50814283,\n",
              "        -1.79168631,  0.23275466,  1.66158004,  1.27139602,  0.1363464 ,\n",
              "         0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.asarray(train_data)\n",
        "np.savetxt(\"train_data.csv\", a, delimiter=\",\")"
      ],
      "metadata": {
        "id": "BmDS0ZVqhhMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, our data for demonstration is ready in a csv file. \n",
        "\n",
        "Let's create `SGDClassifier` object that we intend to train with `partial_fit`."
      ],
      "metadata": {
        "id": "F9G0I4g3R9Ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us create another classifier and we will fit it incrementally.\n",
        "clf2 = SGDClassifier(max_iter=1000, tol=0.01)\n"
      ],
      "metadata": {
        "id": "jTsuMjBDh1TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Processing data chunk by chunk\n",
        "\n",
        "Pandas' `read_csv()` function has an attributre `chunksize` that can be used to read data chunk by chunk. The `chunksize` parameter specifies the number of rows per chunk. (The last chunk may contain fewer than chunksize rows, of course.)\n",
        "\n",
        "We can then use this data for `partial_fit`. We can then repeat these two steps multiple times. That way, entire data may not be reqiuired to be kept in memory."
      ],
      "metadata": {
        "id": "-MstV2rOCKNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "chunksize = 1000\n",
        "\n",
        "iter = 1\n",
        "for train_df in pd.read_csv(\"train_data.csv\", chunksize=chunksize,\n",
        "                            iterator=True):\n",
        "  if iter == 1:\n",
        "    # In the first iteration, we are specifying all possible class \n",
        "    # labels.\n",
        "    xtrain_partial = train_df.iloc[:, 0:10]\n",
        "    ytrain_partial = train_df.iloc[:, 10]\n",
        "    clf2.partial_fit(xtrain_partial, ytrain_partial,\n",
        "                     classes=np.array([0, 1, 2]))\n",
        "  else:\n",
        "    xtrain_partial = train_df.iloc[:, 0:10]\n",
        "    ytrain_partial = train_df.iloc[:, 10]\n",
        "    clf2.partial_fit(xtrain_partial, ytrain_partial)\n",
        "\n",
        "  print(\"After iter #\", iter)\n",
        "  print(clf2.coef_)\n",
        "  print(clf2.intercept_)  \n",
        "  iter = iter + 1"
      ],
      "metadata": {
        "id": "QjPhE1U7huLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e128d6b8-cd14-4e91-df4c-2acfdd5ac57c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After iter # 1\n",
            "[[ -3.92037553  -8.80369826   2.94546802  29.19214586   9.01280493\n",
            "    3.75789677  15.55871439  35.86026859  38.2597941   12.12102894]\n",
            " [-10.82495021  -8.68504032  -9.76804013  -1.99824024   9.0128883\n",
            "   12.90017754   2.52402213  19.03041538  -3.12063155  -2.76948384]\n",
            " [ -3.16503835  10.74980392  -2.84624939 -14.94294568 -15.5253482\n",
            "    8.01377013  -3.8599633  -42.70166356 -19.01602465  -8.49202777]]\n",
            "[-19.66271808 -31.85445465 -18.43709513]\n",
            "After iter # 2\n",
            "[[ -4.23442234  15.33364978   9.51220132  27.73672075   9.75758109\n",
            "    1.92337858 -12.5702007   36.73660429  36.29007357  10.56803131]\n",
            " [  5.55531209  -5.34347297   1.52884562 -10.70366723   9.04689135\n",
            "   -1.35325804   3.88128797  14.40891297 -14.67193102   5.55807907]\n",
            " [  6.77524122 -14.52040094  -0.97362115 -16.89265126 -14.48505254\n",
            "    2.20586224  -4.85366936 -41.43270364 -21.65690535   4.9768124 ]]\n",
            "[-22.46235787 -25.33823759 -12.17728419]\n",
            "After iter # 3\n",
            "[[ -7.46932512 -10.45483468  -1.87311093  16.88919435   9.44214345\n",
            "    0.17911829  -6.60383321  30.17960837  21.91504679   3.97373493]\n",
            " [-15.14910309 -11.1806935   -4.23799713   1.22984772   3.8975513\n",
            "   -6.89923231  -3.81082685   9.3594375    1.42858796  -3.92527222]\n",
            " [  2.0375374   -0.92134791  -2.75866426 -11.00704471  -7.50992739\n",
            "    5.59759413   2.79619728 -22.69469018 -14.21183866   4.92437601]]\n",
            "[-17.46555882 -14.94786523 -15.25140368]\n",
            "After iter # 4\n",
            "[[ -1.05828557  -0.66203645  -0.77825486  13.46374087   5.50766113\n",
            "    4.06973747   0.21459136  19.55304587  17.57546315   3.79371784]\n",
            " [ -1.97449083  -5.87430291  -2.81316832   0.31669622   1.65871301\n",
            "   -1.45400725  -1.66750779   3.87163943   0.33374589   0.26788064]\n",
            " [  0.79929544  -7.66716841  -7.67190339  -7.37168058  -8.73231062\n",
            "    7.9935093    5.34275202 -23.46033998  -9.32510094  -0.59231421]]\n",
            "[-21.02216293 -14.52783706 -10.44924126]\n",
            "After iter # 5\n",
            "[[ -1.38189208  -1.71470494  -0.16676929  10.9820563    5.72698359\n",
            "    1.62396386  -0.90924885  18.70328234  14.27157504  -1.87340237]\n",
            " [  3.47021415  -1.07278585 -10.22860261  -1.48177873   5.08594248\n",
            "   -2.77610588  -1.55850901  10.54768785  -2.23085202   4.42618982]\n",
            " [  2.4600552    2.12626044  -1.88385089 -11.33058012  -5.69595848\n",
            "   -2.13067952  -2.92332334 -18.82212243 -14.73557946   0.32126256]]\n",
            "[-6.45219682 -3.67054092 -7.10985422]\n",
            "After iter # 6\n",
            "[[ -4.3326863    1.58438871  -2.43047702   8.89819381   3.3059463\n",
            "   -2.59406752  -0.16862636  12.17727559  11.63303786   0.55184866]\n",
            " [ -1.6985925   -4.65537127   0.82405808  -1.91163895   0.23382279\n",
            "   -0.04856501  -3.96767846  -0.50982059  -2.54836154  -6.83066041]\n",
            " [  1.76614384  -1.93930316  -2.95231311  -6.44405506  -7.84826539\n",
            "    3.1261229   -1.16449577 -20.98740945  -8.1404735    4.30362098]]\n",
            "[-9.97470717 -7.38319358 -5.89488977]\n",
            "After iter # 7\n",
            "[[  1.24179985  -2.51973421  -1.04968217   9.75228222   2.80422399\n",
            "    2.28533964   1.43501443  11.51874147  12.79229964   0.36614534]\n",
            " [ -1.7611005    6.4309353    0.53962254  -0.97798876   1.47662576\n",
            "   -0.77854848   0.06684831   2.76678487  -1.37443186   0.80726315]\n",
            " [ -0.4132896   -0.19374436  -0.61907284  -6.70798279  -6.8635233\n",
            "    3.1469277    1.79347271 -18.93276518  -8.54193111  -0.56374834]]\n",
            "[-8.56980268 -7.24603076 -7.43979888]\n",
            "After iter # 8\n",
            "[[-1.33198843  1.10230879 -0.76208902  7.12725776  2.28477451  0.57996294\n",
            "   0.95511727  8.94335968  9.33673075 -3.19991664]\n",
            " [-0.94978505  0.1468196   0.81437427 -0.99494481  2.08642197 -2.25899704\n",
            "  -1.197782    4.11815132 -1.42869698  1.73402903]\n",
            " [ 0.49691487 -0.11174574  0.9851301  -4.32437405 -3.16986596 -2.13638081\n",
            "   0.24896996 -9.4056767  -5.57202081 -2.9668316 ]]\n",
            "[-6.29209939 -7.26946591 -6.18104   ]\n",
            "After iter # 9\n",
            "[[ 0.42511131 -0.73104459 -2.98417609  5.27111695  2.99046164 -0.05837494\n",
            "  -1.78435211  9.51626146  6.83741544 -2.89834139]\n",
            " [-0.65534823 -0.90553556 -0.10841001 -2.45298804  2.19829426 -3.49811679\n",
            "   0.23113925  3.58100307 -3.36891766 -4.87905628]\n",
            " [ 2.24882512  0.39830614 -1.02556344 -3.15385514 -3.51063406 -0.6337733\n",
            "  -1.73528778 -9.5343631  -4.00133479 -3.05733114]]\n",
            "[-8.47908061 -6.04684571 -3.14692186]\n",
            "After iter # 10\n",
            "[[ -0.98114221  -0.70862521   1.23634822   5.96787109   2.17770354\n",
            "   -0.12877297   0.28152088   8.07887958   7.80414563  -2.33011221]\n",
            " [  1.8352599    3.32802971   2.90246772   0.05733073   1.97233584\n",
            "    0.57361462   1.62667393   4.43141186  -0.0266947   -1.99863488]\n",
            " [  0.77064221  -2.05979802   2.43870474  -5.85325502  -3.56755152\n",
            "    0.0762846    0.92079277 -11.11792242  -7.57967507  -1.88634518]]\n",
            "[-5.6919661  -3.27246022 -6.96623889]\n",
            "After iter # 11\n",
            "[[ -0.66797423   0.93121612  -0.80615839   4.95039698   4.09875233\n",
            "    1.21291902   2.57514994  11.81591036   6.35417463  -0.57831284]\n",
            " [ -1.36072564  -0.50608198  -4.13712357  -1.89450067   1.44017637\n",
            "   -2.86717978  -0.02683156   2.19092146  -2.58847326   2.65306111]\n",
            " [ -0.07531072   0.31776137  -0.44670622  -2.80595588  -6.15061088\n",
            "   -1.95034595   1.8649071  -15.23668985  -3.40223662   0.15311374]]\n",
            "[-10.10230229  -3.38437041  -2.68414825]\n",
            "After iter # 12\n",
            "[[-1.69385013 -2.1018085   2.35696213  6.47760612  3.15363008 -0.285289\n",
            "  -1.56323983 10.53131664  8.42956906 -1.88968426]\n",
            " [ 1.63813075 -2.35176099 -0.28968532 -2.42273084  1.9932692  -0.61514401\n",
            "  -1.24540168  3.13989834 -3.3180938   1.9836567 ]\n",
            " [ 0.02544878 -0.09358553  1.2317587  -4.54803645 -3.01973535  2.06380092\n",
            "   0.04147652 -9.19140658 -5.87657627  0.98467447]]\n",
            "[-5.32727853 -4.24585091 -5.89583672]\n",
            "After iter # 13\n",
            "[[-0.60541677 -0.06224849 -1.86384409  4.84351066  2.10357508 -0.3922075\n",
            "  -0.89526298  7.30679731  6.31631372 -1.48539583]\n",
            " [ 2.25110942  1.54066623 -0.79466957 -1.00998547  2.17498453 -1.75649544\n",
            "  -0.31928497  4.30762739 -1.45326545  1.93360669]\n",
            " [ 0.37138526  1.40831069 -0.93882847 -2.84716049 -2.93661147  0.7107689\n",
            "  -2.37797195 -8.08817226 -3.62434734  1.64772992]]\n",
            "[-4.62398143 -5.70824672 -5.14861185]\n",
            "After iter # 14\n",
            "[[ 9.36108788e-01 -6.57530689e-01 -3.56090262e-01  4.15647476e+00\n",
            "   2.13965701e+00 -1.60165354e-01 -1.96865975e+00  7.01658155e+00\n",
            "   5.40294038e+00 -1.22996414e+00]\n",
            " [ 1.47304667e+00 -5.39679601e-01  2.91416409e-03 -9.21632374e-01\n",
            "   1.69319275e+00  2.04184375e-01  9.00559481e-01  3.28037667e+00\n",
            "  -1.31094641e+00  1.64325867e+00]\n",
            " [-2.23631405e-01 -1.04767394e+00  1.46831046e+00 -4.05455517e+00\n",
            "  -3.12165970e+00 -6.56580065e-01  8.61247897e-01 -9.15253202e+00\n",
            "  -5.21656240e+00 -1.13127199e+00]]\n",
            "[-5.38181793 -4.9506775  -4.36901641]\n",
            "After iter # 15\n",
            "[[-0.93929888  1.16015686 -1.82143139  5.42930704  2.26212705  0.55223148\n",
            "   0.76030462  7.9766327   7.08523203  0.67453377]\n",
            " [ 0.68127573  0.34506521  1.38098843 -1.8639594  -0.14010278  0.79029847\n",
            "   0.4150217  -1.31835817 -2.46562393 -1.98915814]\n",
            " [ 1.07487532  0.83176844  0.83909557 -4.70440203 -2.61027368 -1.12283672\n",
            "  -2.14908132 -8.36223041 -6.10535949  0.41452371]]\n",
            "[-6.71257904 -4.25608278 -4.34733866]\n",
            "After iter # 16\n",
            "[[-0.54187728 -1.09738984  0.92388107  4.8554968   1.82905311 -0.26857708\n",
            "  -0.99973813  6.70077919  6.34651795 -0.70217465]\n",
            " [ 0.03411054  0.70052184 -0.83789139 -1.01484605  0.6370701  -1.9504475\n",
            "   1.08129853  0.87376513 -1.37959104 -2.4352485 ]\n",
            " [-1.13047483 -1.90179728 -0.10227761 -3.36669865 -3.2362497   1.44907251\n",
            "  -0.12998256 -9.03703276 -4.29801019  0.14479572]]\n",
            "[-4.8464353  -1.88156751 -4.37359803]\n",
            "After iter # 17\n",
            "[[-0.26425927  0.99370853  1.61159689  4.14258778  1.19732798  0.8273022\n",
            "  -2.01391806  4.90665881  5.43361035 -0.89239342]\n",
            " [ 0.89918191  2.3113499  -2.05677076 -0.11675313  0.73065079 -0.70156435\n",
            "   1.36148981  1.56715544 -0.19296266  0.76343463]\n",
            " [-1.10637927  0.61867076 -0.14667515 -2.9721546  -1.84909677 -0.46949675\n",
            "  -0.64043811 -5.72926483 -3.8468354  -0.35749844]]\n",
            "[-3.19344809 -5.34892197 -2.10567427]\n",
            "After iter # 18\n",
            "[[ 1.26643923 -0.03815492  1.65612076  4.11831746  1.04681178  1.00980651\n",
            "  -0.45742721  4.55774623  5.40925247 -0.87822894]\n",
            " [ 0.33293872 -0.37702236 -1.53455729 -1.06791211  1.61984343 -0.69828353\n",
            "   0.25919563  3.03779589 -1.50119499  1.50524791]\n",
            " [ 0.73069203  1.55127602 -1.01316109 -2.91452897 -2.62325102  0.75271347\n",
            "  -1.08762795 -7.42538544 -3.73005099 -0.47140036]]\n",
            "[-3.24761538 -1.57366757 -2.65721091]\n",
            "After iter # 19\n",
            "[[-0.94362187  0.02152925 -0.96975617  4.3821177   1.58788678  0.18036895\n",
            "   0.51009226  5.90728232  5.73104823  1.4378509 ]\n",
            " [-2.73236866 -1.14230264  0.05516982 -1.07725586  1.48468821 -0.40763224\n",
            "   0.26508514  2.7312094  -1.50655    -0.52237161]\n",
            " [-0.16346059 -2.57524749  0.395295   -2.48518695 -2.38504861 -2.15064116\n",
            "  -0.3388152  -6.66226335 -3.17285103  1.41158518]]\n",
            "[-4.30299395 -5.16377616 -3.68301481]\n",
            "After iter # 20\n",
            "[[-0.41383614  1.34289867 -1.57559123  4.14619923  1.84889078 -0.98027354\n",
            "   0.09578863  6.36230872  5.40445626  0.10514178]\n",
            " [ 0.65796783  0.13870888 -0.52604781 -0.07593681  0.06556663 -2.42230746\n",
            "   1.31916895  0.10531086 -0.10416162 -0.50165789]\n",
            " [-0.86207223 -0.65523724 -2.18264875 -2.82441857 -2.0582301  -0.10848726\n",
            "   0.77327198 -6.11614506 -3.63993785 -0.81504014]]\n",
            "[-5.25932727  0.19146897 -2.23361869]\n",
            "After iter # 21\n",
            "[[-0.27441797 -0.46412739  0.18863169  3.59375439  2.18203299 -0.65734246\n",
            "   0.11784889  6.80748795  4.6541691   0.66314267]\n",
            " [-1.06931242 -0.66240119  1.3629818  -1.40911393  1.50924325  1.07248648\n",
            "  -0.12130511  2.60692663 -1.94810684 -0.44233199]\n",
            " [-1.04213159 -0.07034643 -0.35078435 -2.21370057 -2.46534747  0.3319782\n",
            "   1.42893701 -6.69492657 -2.80848542 -1.09263318]]\n",
            "[-4.77686787 -4.05545876 -3.64453174]\n",
            "After iter # 22\n",
            "[[-1.42312552  0.3267405  -0.39607191  3.37028894  1.68317169 -0.55022593\n",
            "  -0.86342278  5.57389908  4.3836864   0.2472894 ]\n",
            " [-0.58616913 -0.87683568  0.85594984 -0.22166079  0.47588793  0.22151243\n",
            "  -1.1437709   0.94214706 -0.31887136 -1.37190451]\n",
            " [ 0.79230046  1.31040226  0.65241224 -3.31962453 -1.44797228 -0.10599355\n",
            "   1.080439   -5.02180785 -4.32872324 -0.58935534]]\n",
            "[-3.43205839 -4.9008511  -3.19941938]\n",
            "After iter # 23\n",
            "[[ 0.7356044  -0.27743432  0.34969304  4.3605459   1.82065787  0.3477671\n",
            "  -0.3871593   6.41497773  5.69030181  0.04617903]\n",
            " [ 0.83668117  0.02659434 -0.92744215 -0.11053878  0.84255403 -0.58203996\n",
            "   1.00454521  1.82017586 -0.19054805 -0.44715873]\n",
            " [-0.27323181  0.44433347 -0.26865677 -2.58791685 -2.58189981 -0.56632292\n",
            "  -1.338021   -7.15688997 -3.2988876   0.63189911]]\n",
            "[-5.54064171 -3.15761274 -1.51632893]\n",
            "After iter # 24\n",
            "[[ 0.31208377  1.13790682 -1.26075862  3.12543025  1.32005505  0.01156115\n",
            "   0.26712742  4.63162643  4.07774913  0.66720065]\n",
            " [-0.37258952  0.6615294  -0.46091763 -0.76317913  0.58649989 -0.60826833\n",
            "  -0.18131195  0.8967351  -1.04306881  0.78178174]\n",
            " [-0.29070978 -1.05518914  0.8368686  -2.81856958 -1.69592446  0.72940734\n",
            "   1.47822105 -5.3046492  -3.65105352  0.02319976]]\n",
            "[-4.31160999 -3.13747312 -3.56021676]\n",
            "After iter # 25\n",
            "[[-0.07854702  0.07352103  0.96903641  3.50107585  1.22183684  0.48215934\n",
            "  -0.87544505  4.61518671  4.5812368   0.29066515]\n",
            " [-0.03239495  0.04790442  0.65250299 -1.00447009  1.32914672 -0.79583221\n",
            "   0.03289759  2.42345543 -1.40188133 -0.8448392 ]\n",
            " [-0.00727819  0.58641829 -1.15685059 -1.50609278 -2.25239465  0.10080091\n",
            "   0.20438978 -5.83798921 -1.8807936   0.12770868]]\n",
            "[-3.54127804 -3.11247096 -2.75667593]\n",
            "After iter # 26\n",
            "[[-0.73799235  1.92977629 -0.56709126  3.3459604   2.10238103  2.04268108\n",
            "   0.50927184  6.49606923  4.32956947 -0.56633813]\n",
            " [ 0.89368015  0.18627687 -1.12860092 -0.71059435  0.33041258 -2.09294613\n",
            "   0.56139371  0.35375318 -0.9599626   1.68126217]\n",
            " [ 1.66221381 -0.4985264  -0.95544829 -2.88905988 -2.04956112 -1.21522128\n",
            "  -0.08555509 -6.13168355 -3.72614938 -0.20723178]]\n",
            "[-3.55818381 -1.96621365 -3.12478406]\n",
            "After iter # 27\n",
            "[[ 0.60647155 -0.38738596  0.75541007  4.10334939  1.92556718  0.19897039\n",
            "  -0.07042454  6.51025999  5.34361247  0.29806059]\n",
            " [-0.02942185 -0.17693801  0.23247162  0.38719608  0.70198884  1.45513878\n",
            "  -1.86742544  1.77513404  0.47712219 -0.65921229]\n",
            " [-0.95220983  0.51770853 -0.96547326 -2.92735951 -1.67703446  0.10447275\n",
            "   0.55760724 -5.32120576 -3.79636973 -0.28677584]]\n",
            "[-3.21417668 -2.70433514 -3.47803019]\n",
            "After iter # 28\n",
            "[[ 0.50870296  0.07460901 -0.39468597  3.35777789  1.14330642 -0.30281653\n",
            "   0.40179811  4.36265531  4.39521408 -0.80119769]\n",
            " [-0.11276018 -0.78921395 -0.73547884  0.01074424  0.38822366 -0.2571244\n",
            "   0.26945298  0.87196296 -0.00597143  0.25808105]\n",
            " [ 1.01675304  0.59327757  0.41206552 -1.94476941 -2.09494203 -0.78590762\n",
            "   0.64617297 -5.72340259 -2.47099082 -1.10400251]]\n",
            "[-4.27649256 -4.09819234 -2.05647362]\n",
            "After iter # 29\n",
            "[[-0.31415349  1.37607928  0.85147589  3.30954754  1.52874949 -0.16153964\n",
            "  -1.01826681  5.19659237  4.31114569  0.29832343]\n",
            " [-0.24487446 -0.77489166 -0.08317874 -0.55733359  0.98577484 -0.64211066\n",
            "   0.34700244  1.89862939 -0.79077425 -1.2507466 ]\n",
            " [ 0.53039662  0.09506966 -0.15461248 -2.10867386 -2.05052064 -1.50485814\n",
            "  -0.06674058 -5.71273537 -2.69075786 -0.72041465]]\n",
            "[-4.59127227 -2.03947475 -2.41782019]\n",
            "After iter # 30\n",
            "[[-0.41516752  0.36305395  1.3411207   3.2209614   1.55707108  0.78482764\n",
            "  -0.0167763   5.2119804   4.19214255  1.02849108]\n",
            " [-0.75576263  0.45414528 -0.55104142 -0.29027395  0.41912188 -0.40798019\n",
            "  -1.1516365   0.77847335 -0.40694332  0.54030427]\n",
            " [-0.61958805  0.17530717 -0.50289583 -1.61192683 -1.67342242  0.19526747\n",
            "  -0.53845263 -4.60335184 -2.05136765  1.12740529]]\n",
            "[-3.89933725 -3.99647369 -2.09253714]\n",
            "After iter # 31\n",
            "[[-0.5727554   0.08618106 -0.3592467   2.24839039  1.28172671  0.12091203\n",
            "  -0.64405546  4.07286965  2.91617337  0.08884216]\n",
            " [ 0.53417341 -1.56432317  1.06499979  0.26116705  0.36023764 -0.90569321\n",
            "   0.10950231  0.94464916  0.32772364 -0.79771962]\n",
            " [-0.28610041 -0.45603105 -0.7227628  -1.23496269 -1.54420624 -0.92814724\n",
            "   0.50648692 -4.11165142 -1.55797964  0.43293152]]\n",
            "[-2.32501064 -3.32368153 -2.09685265]\n",
            "After iter # 32\n",
            "[[ 0.15597039  0.30454149  0.10224635  3.0371221   1.31349479  0.20920411\n",
            "  -0.52216108  4.56933958  3.96093209 -0.98414864]\n",
            " [-1.20396502 -0.72654961 -0.79572878 -0.75761768  0.7761951  -0.65807701\n",
            "  -1.31661877  1.32296481 -1.04557325  0.24504174]\n",
            " [-0.42286302 -0.05945278  0.2789128  -1.94170036 -1.7511905   0.12447817\n",
            "  -0.05353285 -4.95480282 -2.48482803  0.26694869]]\n",
            "[-4.46864822 -2.39220752 -1.49013252]\n",
            "After iter # 33\n",
            "[[-0.72709971  0.3960316  -0.83558385  2.64442568  1.59878317  0.45017874\n",
            "   0.15915857  4.99395094  3.42507696 -0.0433203 ]\n",
            " [ 1.48197362  0.88310474 -0.97118117 -0.77254352  0.56741639  1.8460144\n",
            "  -0.7734495   0.84910499 -1.05449837  0.35239085]\n",
            " [ 0.72381535 -0.26845356 -0.19646109 -1.24097051 -1.96216986  0.91669056\n",
            "  -0.02652064 -5.04741144 -1.54417498 -0.57844782]]\n",
            "[-2.97204363 -2.39140817 -3.26998656]\n",
            "After iter # 34\n",
            "[[ 9.81029096e-01  3.90852782e-02 -3.02479830e-01  2.90361270e+00\n",
            "   1.03832581e+00 -8.69620534e-01  1.18206057e+00  3.88336991e+00\n",
            "   3.79814047e+00  6.15669327e-01]\n",
            " [-7.97546787e-01 -5.99124826e-01  9.69819344e-01 -8.98066574e-02\n",
            "   6.86107154e-01 -1.03596572e-01 -9.96232920e-04  1.48231423e+00\n",
            "  -1.54892003e-01  6.40127835e-01]\n",
            " [-2.36761098e-01  1.55460037e-01  4.62190209e-02 -2.10702991e+00\n",
            "  -1.77425356e+00  5.91845099e-02 -4.18163066e-01 -5.09546907e+00\n",
            "  -2.70296991e+00  8.81425940e-01]]\n",
            "[-2.97646184 -2.39429729 -3.53458733]\n",
            "After iter # 35\n",
            "[[ 0.54304015  0.3070077   0.00813385  2.15485243  0.87156126  0.57051017\n",
            "  -0.63327798  3.10727766  2.81344513 -0.5946874 ]\n",
            " [ 1.35679729  2.22048177 -0.57075885 -0.68690552  0.63422492 -0.26620433\n",
            "  -0.11387225  1.0443709  -0.94436269 -1.44290246]\n",
            " [-0.49276667 -0.44091676 -0.2153295  -2.07175108 -1.63364332  0.32317666\n",
            "  -1.09104314 -4.76271756 -2.66349092  0.0256991 ]]\n",
            "[-2.69397099 -1.83560427 -2.11122804]\n",
            "After iter # 36\n",
            "[[-6.18770934e-01  1.03530989e-01 -8.88441774e-01  2.87151712e+00\n",
            "   1.30228943e+00 -1.09572298e+00  7.31890247e-01  4.45498043e+00\n",
            "   3.74180703e+00  1.94832825e-01]\n",
            " [ 1.59551154e-01  5.27987952e-01  2.15551358e-01 -8.00885309e-01\n",
            "   7.49023146e-01  6.47549599e-01  1.59682563e+00  1.23899468e+00\n",
            "  -1.10156098e+00  2.29047498e-01]\n",
            " [-4.39112215e-01 -5.49124424e-01  4.96000335e-01 -1.87165675e+00\n",
            "  -1.41721762e+00  1.88762610e-03  4.20028384e-01 -4.17188137e+00\n",
            "  -2.40930033e+00 -2.18117079e-01]]\n",
            "[-2.43518203 -3.20594986 -2.12026741]\n",
            "After iter # 37\n",
            "[[-0.18919233 -0.81223984  0.22201669  2.38962674  1.82776048 -0.29894779\n",
            "   0.17803594  5.36733491  3.07510473 -0.06590704]\n",
            " [-1.36179959  0.4555632   0.84063098 -0.61646212  0.77741117 -0.52583426\n",
            "   1.25522803  1.4018442  -0.8583649  -0.32178757]\n",
            " [ 0.07821317 -0.86555037  0.0880872  -1.67590987 -1.6286071  -0.56268954\n",
            "   0.04285776 -4.53788905 -2.13858903 -0.03107696]]\n",
            "[-2.19482679 -3.71764571 -1.592436  ]\n",
            "After iter # 38\n",
            "[[ 0.72846091 -0.19993564  0.35360598  3.35207882  1.24602248  0.280171\n",
            "  -0.62063491  4.58874988  4.38230176 -0.42102343]\n",
            " [-0.32870803 -0.40169214  0.26481211 -0.61172964  0.97131176  0.65649396\n",
            "   0.66598627  1.83700923 -0.86218822  1.24500547]\n",
            " [-0.38326098 -0.36166506 -0.41980067 -2.25332453 -1.43180565 -0.040644\n",
            "  -0.27488582 -4.41037305 -2.91490063 -0.01007806]]\n",
            "[-2.73171079 -3.43581955 -1.85749855]\n",
            "After iter # 39\n",
            "[[-0.15324104  0.19405327  0.28231654  2.63295656  1.27101773 -0.17967046\n",
            "   0.69984005  4.25648513  3.4269369  -0.44893732]\n",
            " [-0.03322146 -0.38490965 -0.4118864  -0.40707868  0.52181414  0.66309469\n",
            "  -0.97130152  0.94456324 -0.56725876 -0.48489147]\n",
            " [-1.54281656 -1.17247596 -0.28633782 -1.61161617 -1.46197761 -0.01840685\n",
            "   1.01989424 -4.13142987 -2.06197144  0.28236719]]\n",
            "[-2.73960576 -2.15629019 -1.60710351]\n",
            "After iter # 40\n",
            "[[-3.31052862e-01  5.46706193e-01  6.59100837e-01  2.65550279e+00\n",
            "   1.23256126e+00  3.39073156e-01 -2.76694543e-01  4.18285067e+00\n",
            "   3.45885260e+00 -1.48559867e-01]\n",
            " [-1.17481230e-01 -5.46773344e-01 -7.77724137e-01 -8.01065329e-01\n",
            "   6.59415226e-01  8.23596528e-01  2.46844918e-01  1.03897340e+00\n",
            "  -1.09713138e+00  9.02817931e-02]\n",
            " [ 8.44317584e-03 -3.27905513e-01  2.03536007e-03 -1.93545259e+00\n",
            "  -1.64083729e+00  2.75147878e-01  1.53727608e-01 -4.70522263e+00\n",
            "  -2.48228831e+00  1.39637001e-01]]\n",
            "[-2.74590208 -2.89190115 -2.34674427]\n",
            "After iter # 41\n",
            "[[-0.30524382 -0.24217538  0.28984579  2.92276318  1.39149017 -0.23170987\n",
            "  -0.02131334  4.68164802  3.80514818  0.16756665]\n",
            " [ 0.2500064  -0.24864808 -0.41194955 -1.05422768  0.29425453 -0.38711005\n",
            "   0.51106032  0.08765966 -1.41397867 -0.44129193]\n",
            " [-0.09020095 -0.11476372  0.69007568 -1.6378571  -1.60325263  0.13200454\n",
            "   0.70719542 -4.4607878  -2.08942519  0.85649879]]\n",
            "[-1.55812069 -1.684087   -2.58456699]\n",
            "After iter # 42\n",
            "[[-0.06907623  0.28324398 -0.16366072  2.48629976  1.47349417  0.05508469\n",
            "   0.02046618  4.6290955   3.22181798 -0.14733152]\n",
            " [-0.2926111  -0.40268055 -0.70083671 -0.92488512  0.64023504 -1.13884075\n",
            "  -0.1431446   0.92936852 -1.26040437  0.52761582]\n",
            " [ 0.68314091  0.03254483 -0.19572728 -1.6203832  -1.85600447  0.67538817\n",
            "   0.11001897 -5.01527347 -2.05307451 -0.62506387]]\n",
            "[-3.45503595 -1.45390942 -2.34067126]\n",
            "After iter # 43\n",
            "[[ 0.30447126  1.60469769 -0.15280938  2.36236573  0.9325389  -0.2916663\n",
            "  -0.58846312  3.35529704  3.08557708 -0.71690527]\n",
            " [-0.05370493 -0.96362656  0.55474648 -1.43067444  0.78548005 -0.4222217\n",
            "   0.5013057   0.98050537 -1.93900431 -0.32154033]\n",
            " [ 0.66733004  0.95701413 -0.02084157 -0.96390135 -1.72165524 -0.41362824\n",
            "   0.13284933 -4.36129558 -1.18911655 -0.63199014]]\n",
            "[-2.52737263 -2.14701012 -1.42089091]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes:** \n",
        "\n",
        "* In the first call to `partial_fit()`, we passed the list of possible target class labels. For subsequent calls to `partial_fit()`, this is not required.\n",
        "\n",
        "* Observe the changing values pf the classifier attributes: `coef_` and `intercept_` which we are printing in each iteration."
      ],
      "metadata": {
        "id": "fvDj_QjRCiC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_score = clf2.score(xtest, ytest)\n",
        "print(\"Test score: \", test_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXvhgqxmmd_S",
        "outputId": "ac42aec7-cea6-4b88-f254-9304e0472167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test score:  0.8244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names\n",
            "  \"X does not have valid feature names, but\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate the classifier by examining the  `confusion_matrix`."
      ],
      "metadata": {
        "id": "4uZPeOVSTCwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ypred = clf2.predict(xtest)\n",
        "cm = confusion_matrix(ytest, ypred)\n",
        "print(cm) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UllH5uoVmkAn",
        "outputId": "4dd32490-0079-4130-9bb3-1d3f53446f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2209  239   98]\n",
            " [ 204 1886  410]\n",
            " [  82  284 2088]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names\n",
            "  \"X does not have valid feature names, but\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cr = classification_report(ytest, ypred)\n",
        "print(cr) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42MCnWnpmo8h",
        "outputId": "e1ae8afa-6530-4b69-dd1f-279fbda0cdd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.87      0.88      2546\n",
            "           1       0.78      0.75      0.77      2500\n",
            "           2       0.80      0.85      0.83      2454\n",
            "\n",
            "    accuracy                           0.82      7500\n",
            "   macro avg       0.82      0.82      0.82      7500\n",
            "weighted avg       0.82      0.82      0.82      7500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apart from `SGDClassifier`, we can also train `Perceptron()`, `MultinomialNB()` and `BernoulliNB()` in a similar manner."
      ],
      "metadata": {
        "id": "evx_DJDO2zth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MGx6UEO2wya6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Incremental Preprocessing Example**"
      ],
      "metadata": {
        "id": "xzhTcLLiwzXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `CountVectorizer` vs `HashingVectorizer`"
      ],
      "metadata": {
        "id": "yoIeBxMixOna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorizers are used to convert a collection of text documents to a vector representation, thus helping in preprocessing them before applying any model on these text documents. \n",
        "\n",
        "`CountVectorizer` and `HashingVectorizer` both perform the task of vectorizing the text documents. However, there are some differences among them. \n",
        "\n",
        "\n",
        "One difference is that `HashingVectorizer` does not store the resulting vocabulary (i.e. the unique tokens). Hence, it can be used to learn from data that does not fit into the computer’s main memory. Each mini-batch is vectorized using `HashingVectorizer` so as to guarantee that the input space of the estimator has always the same dimensionality."
      ],
      "metadata": {
        "id": "yjFzAuqfxlOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With `HashingVectorizer`, each token directly maps to a pre-defined column position in a matrix. For example, if there are 100 columns in the resultant (vectorized) matrix, each token (word) maps to 1 of the 100 columns. The mapping between the word and the position in matrix is done using hashing. "
      ],
      "metadata": {
        "id": "WCcCVR6K19Rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In other words, in `HashingVectorizer`, each token transforms to a column position instead of adding to the vocabulary. Not storing the vocabulary is useful while handling large data sets. This is because holding a huge token vocabulary comprising of millions of words may be a challenege when the memory is limited.\n",
        "\n",
        "Since `HashingVectorizer` does not store vocabulary, its object not only takes lesser space, it also alleviates any dependence with function calls performed on the previous chunk of data in case of incremental learning."
      ],
      "metadata": {
        "id": "lb9f0fru19Ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example\n",
        "\n",
        "Let us take some sample text documents and vectorize them, first using CountVectorizer and then HashingVectorizer."
      ],
      "metadata": {
        "id": "_u0ARuZe5ygv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_documents = ['The well-known saying an apple a day keeps the doctor away has a very straightforward, literal meaning, that the eating of fruit maintains good health.',\n",
        "                  'The proverb first appeared in print in 1866 and over 150 years later is advice that we still pass down through generations.', \n",
        "                  'British apples are one of the nations best loved fruit and according to Great British Apples, we consume around 122,000 tonnes of them each year.', \n",
        "                  'But what are the health benefits, and do they really keep the doctor away?']\n"
      ],
      "metadata": {
        "id": "SSN105bw6XLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. CountVectorizer"
      ],
      "metadata": {
        "id": "WWzPqfr_i88f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first import the library and then create an object of CountVectorizer class. "
      ],
      "metadata": {
        "id": "IVMDp352-2ZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "c_vectorizer = CountVectorizer()"
      ],
      "metadata": {
        "id": "DEKgS7uq7zEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now use this object to vectorize the input text documents using the function `fit_transform()`."
      ],
      "metadata": {
        "id": "HBo98hT4hhBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_c = c_vectorizer.fit_transform(text_documents)"
      ],
      "metadata": {
        "id": "H71qYbPqheQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_c.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e263nl_z76Q6",
        "outputId": "b16e8fc9-252a-4282-efba-02ccdab72bfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 66)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, 66 is the size of the vocabulary.\n",
        "\n",
        "We can also see the volcabulary using `vocabulary_` attribute.\n"
      ],
      "metadata": {
        "id": "6Fk8mHFe_8bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c_vectorizer.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "it0xdVwv8JBq",
        "outputId": "df539883-b74b-442b-95b0-be837b8d6b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'000': 0,\n",
              " '122': 1,\n",
              " '150': 2,\n",
              " '1866': 3,\n",
              " 'according': 4,\n",
              " 'advice': 5,\n",
              " 'an': 6,\n",
              " 'and': 7,\n",
              " 'appeared': 8,\n",
              " 'apple': 9,\n",
              " 'apples': 10,\n",
              " 'are': 11,\n",
              " 'around': 12,\n",
              " 'away': 13,\n",
              " 'benefits': 14,\n",
              " 'best': 15,\n",
              " 'british': 16,\n",
              " 'but': 17,\n",
              " 'consume': 18,\n",
              " 'day': 19,\n",
              " 'do': 20,\n",
              " 'doctor': 21,\n",
              " 'down': 22,\n",
              " 'each': 23,\n",
              " 'eating': 24,\n",
              " 'first': 25,\n",
              " 'fruit': 26,\n",
              " 'generations': 27,\n",
              " 'good': 28,\n",
              " 'great': 29,\n",
              " 'has': 30,\n",
              " 'health': 31,\n",
              " 'in': 32,\n",
              " 'is': 33,\n",
              " 'keep': 34,\n",
              " 'keeps': 35,\n",
              " 'known': 36,\n",
              " 'later': 37,\n",
              " 'literal': 38,\n",
              " 'loved': 39,\n",
              " 'maintains': 40,\n",
              " 'meaning': 41,\n",
              " 'nations': 42,\n",
              " 'of': 43,\n",
              " 'one': 44,\n",
              " 'over': 45,\n",
              " 'pass': 46,\n",
              " 'print': 47,\n",
              " 'proverb': 48,\n",
              " 'really': 49,\n",
              " 'saying': 50,\n",
              " 'still': 51,\n",
              " 'straightforward': 52,\n",
              " 'that': 53,\n",
              " 'the': 54,\n",
              " 'them': 55,\n",
              " 'they': 56,\n",
              " 'through': 57,\n",
              " 'to': 58,\n",
              " 'tonnes': 59,\n",
              " 'very': 60,\n",
              " 'we': 61,\n",
              " 'well': 62,\n",
              " 'what': 63,\n",
              " 'year': 64,\n",
              " 'years': 65}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following is the representation of four text documents."
      ],
      "metadata": {
        "id": "5o_PfRVjAYkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NQZBeWF8BrR",
        "outputId": "0d0ad18b-6006-46ba-cb94-1a8d97d8d951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 54)\t3\n",
            "  (0, 62)\t1\n",
            "  (0, 36)\t1\n",
            "  (0, 50)\t1\n",
            "  (0, 6)\t1\n",
            "  (0, 9)\t1\n",
            "  (0, 19)\t1\n",
            "  (0, 35)\t1\n",
            "  (0, 21)\t1\n",
            "  (0, 13)\t1\n",
            "  (0, 30)\t1\n",
            "  (0, 60)\t1\n",
            "  (0, 52)\t1\n",
            "  (0, 38)\t1\n",
            "  (0, 41)\t1\n",
            "  (0, 53)\t1\n",
            "  (0, 24)\t1\n",
            "  (0, 43)\t1\n",
            "  (0, 26)\t1\n",
            "  (0, 40)\t1\n",
            "  (0, 28)\t1\n",
            "  (0, 31)\t1\n",
            "  (1, 54)\t1\n",
            "  (1, 53)\t1\n",
            "  (1, 48)\t1\n",
            "  :\t:\n",
            "  (2, 39)\t1\n",
            "  (2, 4)\t1\n",
            "  (2, 58)\t1\n",
            "  (2, 29)\t1\n",
            "  (2, 18)\t1\n",
            "  (2, 12)\t1\n",
            "  (2, 1)\t1\n",
            "  (2, 0)\t1\n",
            "  (2, 59)\t1\n",
            "  (2, 55)\t1\n",
            "  (2, 23)\t1\n",
            "  (2, 64)\t1\n",
            "  (3, 54)\t2\n",
            "  (3, 21)\t1\n",
            "  (3, 13)\t1\n",
            "  (3, 31)\t1\n",
            "  (3, 7)\t1\n",
            "  (3, 11)\t1\n",
            "  (3, 17)\t1\n",
            "  (3, 63)\t1\n",
            "  (3, 14)\t1\n",
            "  (3, 20)\t1\n",
            "  (3, 56)\t1\n",
            "  (3, 49)\t1\n",
            "  (3, 34)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### HashingVectorizer\n",
        "Let us now see how `HashingVectorizer` is different from `CountVectorizer`.\n",
        "\n",
        "We will create an object of HashingVectorizer. While creating the object, we need to specify the number of features we wish to have in the feature matrix. "
      ],
      "metadata": {
        "id": "EOL2K5R-hBAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer"
      ],
      "metadata": {
        "id": "k3-3ddNy4oYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us create an object of `HashingVectorizer` class. An important parameter of this class is `n_features`. It declares the number of features (columns) in the output feature matrix. \n",
        "\n",
        "Note: Small numbers of features are likely to cause hash collisions, but large numbers will cause larger coefficient dimensions in linear learners."
      ],
      "metadata": {
        "id": "StgizkZVchF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h_vectorizer= HashingVectorizer(n_features=50) "
      ],
      "metadata": {
        "id": "PyLVRM_k8Q31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's perform hashing vectorization with `fit_transform`."
      ],
      "metadata": {
        "id": "GYToqg3HTzpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_h = h_vectorizer.fit_transform(text_documents)\n"
      ],
      "metadata": {
        "id": "qxL3ibCk8mkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us examine the shape of the transformed feature matrix. The number of columns in this matrix is equal to the `n_features` attribute we specified."
      ],
      "metadata": {
        "id": "FKTvDM0TT6Dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_h.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxWPDdOr8o4P",
        "outputId": "6ef8fe57-52d6-429f-9191-3c54f0453afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print the representation of the first example."
      ],
      "metadata": {
        "id": "JGtpk01dUBwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_h[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMRrrBAI8r0E",
        "outputId": "71fd8230-2aae-4667-aee8-57520651bc7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 5)\t0.0\n",
            "  (0, 8)\t-0.47140452079103173\n",
            "  (0, 10)\t-0.23570226039551587\n",
            "  (0, 11)\t-0.23570226039551587\n",
            "  (0, 13)\t0.0\n",
            "  (0, 18)\t-0.23570226039551587\n",
            "  (0, 20)\t0.23570226039551587\n",
            "  (0, 26)\t0.0\n",
            "  (0, 29)\t0.23570226039551587\n",
            "  (0, 33)\t0.23570226039551587\n",
            "  (0, 36)\t-0.23570226039551587\n",
            "  (0, 38)\t0.47140452079103173\n",
            "  (0, 39)\t-0.23570226039551587\n",
            "  (0, 45)\t-0.23570226039551587\n",
            "  (0, 46)\t0.23570226039551587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, `HashingVectorizer` is a good choice if we are falling short of memory and resources, or we need to perform incremental learning. However, `CountVectorizer` is a good choice if we need to access the actual tokens."
      ],
      "metadata": {
        "id": "OYrq7Zxy0GvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "11TZJOBr19cO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Combining preprocessing and fitting in Incremental Learning**\n",
        "\n",
        "###(`HashingVectorizer` along with `SGDClassifier`)"
      ],
      "metadata": {
        "id": "UXnSmCop9cQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "We will now use a dataset containing a textual feature that requires preprocessing using a vectorizer. Since we wish to perform incremental learning using `partial_fit()`, we will preprocess (i.e., vectorize) the dataset feature using `HashingVectorizer` and then we will incrementally fit it. "
      ],
      "metadata": {
        "id": "ePshEoxdoUy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Downloading the dataset\n",
        "\n",
        "Below, we download a dataset from UCI ML datasets' library. (Instead of downloading, unzipping and then reading, we are directly reading the zipped csv file. For that purpose, we are making use of `urllib.request`, `BytesIO` and `TextIOWrapper` classes.)\n",
        "\n",
        "This is a sentiment analysis dataset. There are only two columns in the dataset. One for the textual review and the other for the sentiment."
      ],
      "metadata": {
        "id": "tpuqo-ItrzeE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC1GLaKtgpIG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from io import StringIO, BytesIO, TextIOWrapper\n",
        "from zipfile import ZipFile\n",
        "import urllib.request\n",
        "\n",
        "resp = urllib.request.urlopen('https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip')\n",
        "zipfile = ZipFile(BytesIO(resp.read()))\n",
        "\n",
        "data = TextIOWrapper(zipfile.open('sentiment labelled sentences/amazon_cells_labelled.txt'), encoding='utf-8')\n",
        "\n",
        "df = pd.read_csv(data, sep = '\\t')\n",
        "df.columns = ['review', 'sentiment']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Exploring the data set."
      ],
      "metadata": {
        "id": "F9e8KHH7uwZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore the dataset a bit."
      ],
      "metadata": {
        "id": "nNUKX--iuFr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rtDjBulzgyC3",
        "outputId": "d59e472f-c9e0-4aa2-973b-24a94cff595b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4dd6cf18-3402-4dad-9084-7f08d8360c94\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Good case, Excellent value.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Great for the jawbone.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tied to charger for conversations lasting more...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The mic is great.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I have to jiggle the plug to get it to line up...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4dd6cf18-3402-4dad-9084-7f08d8360c94')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4dd6cf18-3402-4dad-9084-7f08d8360c94 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4dd6cf18-3402-4dad-9084-7f08d8360c94');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0                        Good case, Excellent value.          1\n",
              "1                             Great for the jawbone.          1\n",
              "2  Tied to charger for conversations lasting more...          0\n",
              "3                                  The mic is great.          1\n",
              "4  I have to jiggle the plug to get it to line up...          0"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3rHSVfyZgzyW",
        "outputId": "429717db-1321-4026-fb64-631b663088e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-12c44781-337b-40ef-9ba1-99cba4c37651\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>The screen does get smudged easily because it ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>What a piece of junk.. I lose more calls on th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>Item Does Not Match Picture.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>The only thing that disappoint me is the infra...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>You can not answer calls with the unit, never ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-12c44781-337b-40ef-9ba1-99cba4c37651')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-12c44781-337b-40ef-9ba1-99cba4c37651 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-12c44781-337b-40ef-9ba1-99cba4c37651');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                review  sentiment\n",
              "994  The screen does get smudged easily because it ...          0\n",
              "995  What a piece of junk.. I lose more calls on th...          0\n",
              "996                       Item Does Not Match Picture.          0\n",
              "997  The only thing that disappoint me is the infra...          0\n",
              "998  You can not answer calls with the unit, never ...          0"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy9iYmn5g1ax",
        "outputId": "850de0b2-7708-4b82-b969-5b65dba70879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 999 entries, 0 to 998\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     999 non-null    object\n",
            " 1   sentiment  999 non-null    int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 15.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "IUMKr24eg3-E",
        "outputId": "c22ff35c-6365-4db9-a8e3-b4871b5d2a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5d45396c-84ba-4f61-81c4-0879bf39ade3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>999.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.500501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.500250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5d45396c-84ba-4f61-81c4-0879bf39ade3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5d45396c-84ba-4f61-81c4-0879bf39ade3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5d45396c-84ba-4f61-81c4-0879bf39ade3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        sentiment\n",
              "count  999.000000\n",
              "mean     0.500501\n",
              "std      0.500250\n",
              "min      0.000000\n",
              "25%      0.000000\n",
              "50%      1.000000\n",
              "75%      1.000000\n",
              "max      1.000000"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[:, 'sentiment'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ3ucClIuVwQ",
        "outputId": "684717b0-e493-4d08-8d2e-66a62c74ed27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, \n",
        "- There are 999 samples in the dataset. \n",
        "- The possible classes for sentiment are 1 and 0."
      ],
      "metadata": {
        "id": "l9MedU1SuMAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Splitting data into train and test"
      ],
      "metadata": {
        "id": "DDmROb6tvLtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "TJhP7_DOvPIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.loc[:, 'review']"
      ],
      "metadata": {
        "id": "rxv-Eaq6iDR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y= df.loc[:, 'sentiment']"
      ],
      "metadata": {
        "id": "vkQUEKLYiHb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
      ],
      "metadata": {
        "id": "J_5oPFxPh708"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8UvlKb-iNeL",
        "outputId": "cca787df-92b2-4826-e12e-6479c1779429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(799,)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGdHnhpCiOrs",
        "outputId": "5b8188ca-96f5-41fe-9283-1a2f741bdef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(799,)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Preprocessing\n",
        "\n",
        "Since the data is textual, we need to vectorize it. In order to perform incremental learning, we will use HashingVectorizer."
      ],
      "metadata": {
        "id": "xjpjGIwku0gK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "vectorizer = HashingVectorizer()"
      ],
      "metadata": {
        "id": "5WYLQv5hhGV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Creating an instance of the SGDClassifier"
      ],
      "metadata": {
        "id": "X7bs-NBWvZ_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "classifier = SGDClassifier(penalty='l2',loss='hinge')"
      ],
      "metadata": {
        "id": "6hQSNiySvWIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Iteration 1 of partial_fit()\n",
        "\n",
        "We will assume we do not have sufficient memory to handle all the 799 samples in one go for training purpose. So, we will take the first 400 samples from teh training data and `partial_fit` our classifier.\n",
        "\n",
        "Another use case of partial_fit here could also be a scenario where we only have 400 samples available at a time. So, we fit our classifier with them. However, we `partial_fit` it, to have the possibility of training it wirth more data later whenever that becomes available.\n"
      ],
      "metadata": {
        "id": "EGB1tkcqilOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_part1_hashed = vectorizer.fit_transform(X_train[0:400])\n",
        "y_train_part1 = y_train[0:400]\n"
      ],
      "metadata": {
        "id": "SPPH_cuKhSCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_classes = np.unique(df.loc[:, 'sentiment']) #we need to mention all classes in the first iteration of partial_fit()"
      ],
      "metadata": {
        "id": "wAq3-VDXlvVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.partial_fit(X_train_part1_hashed, y_train_part1, classes=all_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU_Eev01lpUR",
        "outputId": "d5d9efdd-70f8-4606-ebb8-bd0352f13f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now use this classifier on our test data that we had kept aside earlier."
      ],
      "metadata": {
        "id": "YwKxGLSGwWwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_hashed = vectorizer.transform(X_test) #first we will have to preprocess the X_test with the same vectorizer that was fit on train data."
      ],
      "metadata": {
        "id": "5ch0QlO8mxuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_score = classifier.score(X_test_hashed, y_test)\n",
        "print(\"Test score: \", test_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2skZ8Jfm7Fb",
        "outputId": "690ea265-6a20-44e0-a65c-453790bf19ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test score:  0.705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: We can also store this classifier using pickle object and can access it later."
      ],
      "metadata": {
        "id": "cTyX3Gi-wvbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Iteration 2 of partial_fit()\n",
        "\n",
        "We will now assume that more data became available. So, we will fit the same classifier with more data and observe if our test score improves."
      ],
      "metadata": {
        "id": "xpeML2DVmEcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_part2_hashed = vectorizer.fit_transform(X_train[400:])\n",
        "y_train_part2 = y_train[400:]\n"
      ],
      "metadata": {
        "id": "-VdFDiP3mJI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.partial_fit(X_train_part2_hashed, y_train_part2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de60f2e6-a5aa-439c-8aa1-96620ac26273",
        "id": "1JfS-Q5KmJJB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_score = classifier.score(X_test_hashed, y_test)\n",
        "print(\"Test score: \", test_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSQZGUpemC4p",
        "outputId": "6ac2097a-c2f5-4601-a7c9-7ab86a8a4832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test score:  0.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that our test score has improved after we fed more data to the classifier in the second iteration of `partial_fit()`."
      ],
      "metadata": {
        "id": "W3OUBNtRxAnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a more elaborate example, refer: https://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py"
      ],
      "metadata": {
        "id": "X923v4Dz1rS5"
      }
    }
  ]
}