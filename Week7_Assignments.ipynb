{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqFJHtGrtnqklj511E7X9o"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Practice Assignment 7.1\n",
        "Q1) Vectorize the given text data using countvectorizer() object. What is the shape of text_data after vectorization?"
      ],
      "metadata": {
        "id": "1IxpOmcDfUSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "yAAVnMiR2rG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pM6P63EOdcRx"
      },
      "outputs": [],
      "source": [
        "text_data=['A metaverse is a network of 3D virtual worlds focused on social connection.',\n",
        "           'In futurism and science fiction, the term is often described as a hypothetical iteration of the Internet as a single', \n",
        "           'universal virtual world that is facilitated by the use of virtual and augmented reality headsets.',\n",
        "           'The term \"metaverse\" has its origins  the 1992 science fiction novel Snow Crash as a portmanteau of \"meta\" and \"universe.\"',\n",
        "           'Various metaverses have been developed for popular use such as virtual world platforms like Second Life.',\n",
        "           'Some metaverse iterations involve integration between virtual and physical spaces and virtual economies',\n",
        "           'often including a significant interest in advancing virtual reality technology.', \n",
        "           'The term has seen considerable use as a buzzword for public relations purposes to exaggerate development progress for various related technologies and projects.[10] Information privacy and user addiction are concerns within metaverses',\n",
        "           'stemming from challenges facing the social media and video game industries as a whole.']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cnt = CountVectorizer()\n",
        "Text_Count = cnt.fit_transform(text_data)\n",
        "Text_Count.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSUgNavFfi69",
        "outputId": "102d091e-1723-4e2a-d5c6-ce4a675422ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, 99)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Text_Count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpMYvhUGgeAQ",
        "outputId": "71a17e7d-4827-41eb-8a10-be24a3f46725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 51)\t1\n",
            "  (0, 43)\t1\n",
            "  (0, 53)\t1\n",
            "  (0, 55)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 94)\t1\n",
            "  (0, 98)\t1\n",
            "  (0, 26)\t1\n",
            "  (0, 57)\t1\n",
            "  (0, 77)\t1\n",
            "  (0, 15)\t1\n",
            "  (1, 43)\t1\n",
            "  (1, 55)\t1\n",
            "  (1, 35)\t1\n",
            "  (1, 29)\t1\n",
            "  (1, 5)\t1\n",
            "  (1, 71)\t1\n",
            "  (1, 25)\t1\n",
            "  (1, 86)\t2\n",
            "  (1, 84)\t1\n",
            "  (1, 56)\t1\n",
            "  (1, 18)\t1\n",
            "  (1, 7)\t2\n",
            "  (1, 34)\t1\n",
            "  (1, 44)\t1\n",
            "  :\t:\n",
            "  (7, 64)\t1\n",
            "  (7, 69)\t1\n",
            "  (7, 82)\t1\n",
            "  (7, 65)\t1\n",
            "  (7, 0)\t1\n",
            "  (7, 38)\t1\n",
            "  (7, 63)\t1\n",
            "  (7, 91)\t1\n",
            "  (7, 3)\t1\n",
            "  (7, 6)\t1\n",
            "  (7, 14)\t1\n",
            "  (7, 96)\t1\n",
            "  (8, 77)\t1\n",
            "  (8, 5)\t1\n",
            "  (8, 86)\t1\n",
            "  (8, 7)\t1\n",
            "  (8, 80)\t1\n",
            "  (8, 28)\t1\n",
            "  (8, 13)\t1\n",
            "  (8, 24)\t1\n",
            "  (8, 49)\t1\n",
            "  (8, 93)\t1\n",
            "  (8, 30)\t1\n",
            "  (8, 37)\t1\n",
            "  (8, 95)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnt.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwU4XMvGgERH",
        "outputId": "b4362181-0697-4b05-eee5-60f6cd230037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'metaverse': 51,\n",
              " 'is': 43,\n",
              " 'network': 53,\n",
              " 'of': 55,\n",
              " '3d': 2,\n",
              " 'virtual': 94,\n",
              " 'worlds': 98,\n",
              " 'focused': 26,\n",
              " 'on': 57,\n",
              " 'social': 77,\n",
              " 'connection': 15,\n",
              " 'in': 35,\n",
              " 'futurism': 29,\n",
              " 'and': 5,\n",
              " 'science': 71,\n",
              " 'fiction': 25,\n",
              " 'the': 86,\n",
              " 'term': 84,\n",
              " 'often': 56,\n",
              " 'described': 18,\n",
              " 'as': 7,\n",
              " 'hypothetical': 34,\n",
              " 'iteration': 44,\n",
              " 'internet': 41,\n",
              " 'single': 75,\n",
              " 'universal': 88,\n",
              " 'world': 97,\n",
              " 'that': 85,\n",
              " 'facilitated': 23,\n",
              " 'by': 12,\n",
              " 'use': 90,\n",
              " 'augmented': 8,\n",
              " 'reality': 68,\n",
              " 'headsets': 33,\n",
              " 'has': 31,\n",
              " 'its': 46,\n",
              " 'origins': 58,\n",
              " '1992': 1,\n",
              " 'novel': 54,\n",
              " 'snow': 76,\n",
              " 'crash': 17,\n",
              " 'portmanteau': 62,\n",
              " 'meta': 50,\n",
              " 'universe': 89,\n",
              " 'various': 92,\n",
              " 'metaverses': 52,\n",
              " 'have': 32,\n",
              " 'been': 9,\n",
              " 'developed': 19,\n",
              " 'for': 27,\n",
              " 'popular': 61,\n",
              " 'such': 81,\n",
              " 'platforms': 60,\n",
              " 'like': 48,\n",
              " 'second': 72,\n",
              " 'life': 47,\n",
              " 'some': 78,\n",
              " 'iterations': 45,\n",
              " 'involve': 42,\n",
              " 'integration': 39,\n",
              " 'between': 10,\n",
              " 'physical': 59,\n",
              " 'spaces': 79,\n",
              " 'economies': 21,\n",
              " 'including': 36,\n",
              " 'significant': 74,\n",
              " 'interest': 40,\n",
              " 'advancing': 4,\n",
              " 'technology': 83,\n",
              " 'seen': 73,\n",
              " 'considerable': 16,\n",
              " 'buzzword': 11,\n",
              " 'public': 66,\n",
              " 'relations': 70,\n",
              " 'purposes': 67,\n",
              " 'to': 87,\n",
              " 'exaggerate': 22,\n",
              " 'development': 20,\n",
              " 'progress': 64,\n",
              " 'related': 69,\n",
              " 'technologies': 82,\n",
              " 'projects': 65,\n",
              " '10': 0,\n",
              " 'information': 38,\n",
              " 'privacy': 63,\n",
              " 'user': 91,\n",
              " 'addiction': 3,\n",
              " 'are': 6,\n",
              " 'concerns': 14,\n",
              " 'within': 96,\n",
              " 'stemming': 80,\n",
              " 'from': 28,\n",
              " 'challenges': 13,\n",
              " 'facing': 24,\n",
              " 'media': 49,\n",
              " 'video': 93,\n",
              " 'game': 30,\n",
              " 'industries': 37,\n",
              " 'whole': 95}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3) What is the token associated with word \"metaverse\"?"
      ],
      "metadata": {
        "id": "vQ0ZKNFRja9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnt.vocabulary_['metaverse']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdFljIhYgTRM",
        "outputId": "137d09d8-652c-43ad-af11-ccc113b9dd74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2) What is the shape of text_data after vectorization after the term that appeared in less than 2 documents are ignored?"
      ],
      "metadata": {
        "id": "_rLvtLE4gf7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnt1 = CountVectorizer(min_df=2)\n",
        "Text_Count = cnt1.fit_transform(text_data)\n",
        "Text_Count.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDrTOj0Kg7wv",
        "outputId": "9673b410-b4da-4d5a-da46-9845b5a31c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4) Docs = ['This is the first question.', 'This document is the second document.', 'And this is the third one' ]\n",
        "* Write a code to convert the above text into an array after tokenizing it using countvectorizer with 10 features. What will be the output?"
      ],
      "metadata": {
        "id": "hhxWxfL_jiPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = ['This is the first question.', 'This document is the second document.', 'And this is the third one' ]\n",
        "cnt2 = CountVectorizer(max_features=10)\n",
        "docs_count = cnt2.fit_transform(docs)\n",
        "print(docs_count.shape)\n",
        "print(docs_count.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLQkbvAqjpEB",
        "outputId": "3d1dae9e-70e5-458e-f289-ddc107cb9954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 10)\n",
            "[[0 0 1 1 0 1 0 1 0 1]\n",
            " [0 2 0 1 0 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 0 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5) Define a function called k_closest in python with the following signature:\n",
        "* Which returns closest vectors in X which are closest to the vector p. Use the appropriate sklearn methods.\n",
        "* Consider, X = np.asarray([[72, 69 ,82], [ 9 ,79, 99], [20 ,47, 88], [80 ,64, 49]]) p= np.asarray([[0,0,0]]) and k=1 For these values, what is the output of the function?"
      ],
      "metadata": {
        "id": "R2tXXnnwkmfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def k_closest(X, p):\n",
        "  knn = NearestNeighbors(n_neighbors=1)\n",
        "  knn.fit(X)\n",
        "  return knn.kneighbors(p)\n",
        "\n",
        "X = np.asarray([[72, 69 ,82], [ 9 ,79, 99], [20 ,47, 88], [80 ,64, 49]]) \n",
        "p= np.asarray([[0,0,0]])\n",
        "dist, indx = k_closest(X, p)\n",
        "print(X[indx[0][0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKVES56wkt83",
        "outputId": "7a9375e0-55f1-47ca-b0be-9cac09c04c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20 47 88]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6) Define a function called distance in python with the following signature: def distance( x0 : \"A vector as np.array\", X: \"A list of vectors as np.ndarray\", p : \"p value for Minkowsky distance. Default 2, for Eucledian\"):    which returns a list of calculated Minkowsky distances between the point x0 and all points in X. Recall that the p-Minkowsky distance between\n",
        "$$  \n",
        "x=(x1,x2,…xn)^T and \\space y=(y1,y2,…yn)^T\n",
        "$$\n",
        "$$\n",
        "δ(x,y) = (|x1−y1|^p+|x2−y2|^p+⋯+|xn−yn|^p)^{\\frac{1}{p}}\n",
        "​$$\n",
        "Do not use any predefined function to calculate the same. \n",
        "\n",
        "Consider, X0 = np.array([1,0,0,0]), X= np.asarray([[1, 0, 0,0], [0, 1, 1,1],[1,2,0,0]]), p= 2 then, what is the output of the function distance(X0,X,p) ?"
      ],
      "metadata": {
        "id": "ywB-bg1PnsAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def distance(x0, X, p=2):\n",
        "  n = len(x0)\n",
        "  dist = []\n",
        "  for Xi in X:\n",
        "    dist.append([])\n",
        "    d = 0\n",
        "    for i in range(n):\n",
        "      d += (Xi[i] - x0[i])**p\n",
        "    dist[-1].append(d**(1/p))\n",
        "  return dist\n",
        "\n",
        "X0 = np.array([1,0,0,0])\n",
        "X = np.asarray([[1, 0, 0,0], [0, 1, 1,1],[1,2,0,0]])\n",
        "p= 2\n",
        "print(distance(X0, X, p))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWgMEAnpnrgF",
        "outputId": "eced9daa-319d-4200-9adb-a158f0fe6c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0], [2.0], [2.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7 to Q11\n",
        "Load digit dataset on Colab (datasets.load_digits()) and use softmax regression to build the model Using following steps.\n",
        "* Step 1: Load the dataset and split it using train_test_split by keeping: test_size= 0.2 random_state=10\n",
        "* Step 2: Use standard scaler as a scaling function to scale the training as well as test data.\n",
        "* Step 3: Use Logisticregression() as an estimator and set parameter (multi_class='multinomial', solver='sag') to predict the output."
      ],
      "metadata": {
        "id": "gspDje3eqxgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "digits_data, digits_label = load_digits(return_X_y=True)\n",
        "# type(digits_data)\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits_data, digits_label, test_size=0.2, random_state=10)\n",
        "pipe = Pipeline([('scaler', StandardScaler()),\n",
        "                 ('logreg', LogisticRegression(multi_class='multinomial', solver='sag'))\n",
        "                 ])\n"
      ],
      "metadata": {
        "id": "1Od7w47qq57M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7) What is the shape of features data you got in digits dataset ?"
      ],
      "metadata": {
        "id": "lLXwU5f-svSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "digits_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbGR1xvWsxI2",
        "outputId": "56bc5b48-cf88-4a97-8f1c-3cf3492dbf52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8) How many labels are there in the dataset?"
      ],
      "metadata": {
        "id": "siUtp4g0s5I_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(np.unique(digits_label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tOknhUTs71c",
        "outputId": "9a4d3d7d-49e5-47f4-9126-5beb78106c35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9) How many times label '7' is there in the dataset?"
      ],
      "metadata": {
        "id": "tewRMsL7tILU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(digits_label[digits_label == 7])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoYVO-AmtL0i",
        "outputId": "78bd3309-20bb-45fd-9eb8-21a48b986cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 10) What is the accuracy of the trained model on test data?"
      ],
      "metadata": {
        "id": "Fp1xedB3um0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.fit(X_train, y_train)\n",
        "score = pipe.score(X_test, y_test)\n",
        "print(score)\n",
        "print(classification_report(y_test, pipe.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw3VvWM3uqVh",
        "outputId": "e6c85a7b-e07e-485f-ecac-751fa084d07b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9666666666666667\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        37\n",
            "           1       0.97      0.91      0.94        34\n",
            "           2       0.89      1.00      0.94        34\n",
            "           3       1.00      0.95      0.97        40\n",
            "           4       1.00      0.94      0.97        34\n",
            "           5       0.94      1.00      0.97        32\n",
            "           6       0.97      0.97      0.97        37\n",
            "           7       1.00      1.00      1.00        40\n",
            "           8       0.89      0.94      0.91        33\n",
            "           9       1.00      0.95      0.97        39\n",
            "\n",
            "    accuracy                           0.97       360\n",
            "   macro avg       0.97      0.97      0.97       360\n",
            "weighted avg       0.97      0.97      0.97       360\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11) Write the f1_score value you got .(Keep parameter average='weighted')\n",
        "\n",
        "0.97"
      ],
      "metadata": {
        "id": "yEZCd4txvqZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice Assignment 7.2\n",
        "\n",
        "Q1) Write a function compute_score(X_train, y_train, X_test, y_test)  to do the following on the Iris dataset-\n",
        "\n",
        "Write your code keeping in mind:\n",
        "* Split the Iris dataset into train and test set with 70:30 ratio\n",
        "* Import svm.SVC as 'model'\n",
        "* kernel as 'rbf', regularization parameter as 20 and gamma as 'auto'\n",
        "* Take random_state=42\n",
        "\n",
        "Train the model using trining data and predict the computed model's score using test data.\n",
        "* Which of the following options is the computed score?"
      ],
      "metadata": {
        "id": "l6601b6IwiFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "def compute_score(X_train, y_train, X_test, y_test):\n",
        "  model = SVC(kernel='rbf', C=20, gamma='auto', random_state=42)\n",
        "  model.fit(X_train, y_train)\n",
        "  score = model.score(X_test, y_test)\n",
        "  return score"
      ],
      "metadata": {
        "id": "-mSGT2ZDf_Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "# iris_data, iris_label = load_iris(return_X_y=True)\n",
        "#Define the column names\n",
        "col_name=[\"sepal_length_in_cm\", \"sepal_width_in_cm\",\"petal_length_in_cm\",\"petal_width_in_cm\", \"class\"]\n",
        "#Read the dataset\n",
        "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", header = None, names= col_name)\n",
        "# print(df.columns)\n",
        "iris_data = df[df.columns[:-1]]\n",
        "iris_label = df[df.columns[-1]]\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.3, random_state=42)\n",
        "score = compute_score(X_train, y_train, X_test, y_test)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaJuQMokgj1C",
        "outputId": "9432d2a4-a737-441c-b56f-7f83c279ee5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2) In 'Question 1', apply a pipeline containing a MinMaxScaler()function calledScaler and a svm.svc() called classifierwith the parameters : kernel='linear', decision_function_shape='ovr', C=1, class_weight=None. \n",
        "\n",
        "Calculate the precision value and f1 score and mark the correct option from the list below."
      ],
      "metadata": {
        "id": "yHPo-UiQkRtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "pipe = Pipeline([('scaler', MinMaxScaler()),\n",
        "                 ('svc', SVC(kernel='linear', decision_function_shape='ovr', C=1, class_weight=None))\n",
        "                 ])\n",
        "pipe.fit(X_train, y_train)\n",
        "y_pred = pipe.predict(X_test)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)\n",
        "precision = precision_score(y_test, y_pred, average=None)\n",
        "print('Precision score: ', precision)\n",
        "f1 = f1_score(y_test, y_pred, average=None)\n",
        "print('F1 score: ', f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKlExnunkTxi",
        "outputId": "7c064c34-ed04-4c2e-cf95-97246346c42a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "    Iris-setosa       1.00      1.00      1.00        19\n",
            "Iris-versicolor       1.00      1.00      1.00        13\n",
            " Iris-virginica       1.00      1.00      1.00        13\n",
            "\n",
            "       accuracy                           1.00        45\n",
            "      macro avg       1.00      1.00      1.00        45\n",
            "   weighted avg       1.00      1.00      1.00        45\n",
            "\n",
            "Precision score:  [1. 1. 1.]\n",
            "F1 score:  [1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3) Import the iris dataset and drop the rows where class=Iris-versicolor. Apply a pipeline containing a MinMaxScaler()function calledScaler and a svm.svc() called classifier. Split the iris dataset into 50:50 ratio with random_state=0. Mark the correct recall value from the given options."
      ],
      "metadata": {
        "id": "Xi9JRpIpl52t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "iris_new = iris_data[iris_label != 'Iris-versicolor']\n",
        "label_new = iris_label[iris_label != 'Iris-versicolor']\n",
        "iris_train, iris_test, label_train, label_test = train_test_split(iris_new, label_new, test_size=0.5, random_state=0)\n",
        "np.unique(iris_label)\n",
        "pipe1 = Pipeline([('Scaler', MinMaxScaler()),\n",
        "                  ('classifier', SVC())\n",
        "                  ])\n",
        "pipe1.fit(iris_train, label_train)\n",
        "y_pred1 = pipe1.predict(X_test)\n",
        "recall = recall_score(y_test, y_pred, average='micro')\n",
        "print(recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTGNd0_ml8lF",
        "outputId": "f7af1d6a-28b6-43ce-9ff1-e109e5d4ce51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4) Write the function compute_score(X_train, y_train, X_test, y_test)\n",
        " to do the following on the Iris dataset-\n",
        "\n",
        "Write your code keeping in mind:\n",
        "\n",
        "* Split the Iris dataset into train and test set with 70:30 ratio and random_state=42\n",
        "* Import sklearn.svm.LinearSVC as 'model'\n",
        "* Consider loss function loss=hinge, random_state=42 and penalty=l2\n",
        "\n",
        "Train the 'model' and mark the computed 'score'"
      ],
      "metadata": {
        "id": "UvAmgjG1q3Zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "def compute_score(X_train, y_train, X_test, y_test):\n",
        "  model = LinearSVC(loss='hinge', penalty='l2', random_state=42)\n",
        "  model.fit(X_train, y_train)\n",
        "  score = model.score(X_test, y_test)\n",
        "  return score"
      ],
      "metadata": {
        "id": "KLRm5fLvrfwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris_data, iris_label = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.3, random_state=42)\n",
        "score = compute_score(X_train, y_train, X_test, y_test)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX-bUeyxr3ip",
        "outputId": "e7f4b534-2fce-4f1b-ee03-b16a01a7dddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5) Write a function hyperparameter_search which accepts the Kernel and regularization parameter as inputs and returns the avg_score of the models with the below mentioned hyperparameters.\n",
        "\n",
        "* Split the Iris dataset into train and test set with 70:30 ratio\n",
        "* kernels = ['linear', 'rbf']\n",
        "* Regularization C = [5, 10,100]\n",
        "* Cross Validation = 10\n",
        "* random_state=42\n",
        "\n",
        "Which of the following options give the accuracy_score for the iris dataset?"
      ],
      "metadata": {
        "id": "gK1HCA76sW_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "def hyperparameter_search(kernel, regularization):\n",
        "  param_grid = dict(kernel=kernel, C=regularization)\n",
        "  model = GridSearchCV(SVC(kernel = 'rbf'), scoring='accuracy', param_grid=param_grid, cv=10)\n",
        "  model.fit(X_train, y_train) \n",
        "  best_est = model.best_estimator_\n",
        "  print(best_est)\n",
        "  score = best_est.score(X_test, y_test)\n",
        "  print(score)"
      ],
      "metadata": {
        "id": "Fa72n5autFD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris_data, iris_label = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.3, random_state=42)\n",
        "kernel = ['linear', 'rbf']\n",
        "regularization = [5, 10,100]\n",
        "hyperparameter_search(kernel, regularization)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvkxkoXTtAqG",
        "outputId": "8db4fd1b-b1ee-42a7-e3fd-6b7eb2501a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVC(C=5, kernel='linear')\n",
            "0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Graded Assignment 7 (Part A)\n",
        "\n",
        "Step 1: Download the dataset using following link: (https://drive.google.com/file/d/1v-uxWEgTI0GDCOTZOX3shUMkTf1a_CL7/view?usp=sharing)\n",
        "\n",
        "Step 2: Import the data in google colab using pd.read_csv().\n",
        "\n",
        "Step 3: Seperate features and target data in seperate variable X and Y.\n",
        "\n",
        "Step 4: Convert dataframe X and series y into array and save it in variable X_array,y_array.\n",
        "\n",
        "Step 5: Split the dataset using train_test_split. (Keep parameter train_size=0.7 and random_state=10).\n",
        "\n",
        "Step 6: Reshape the dataset in such a way that each entry of data has 90 samples.\n",
        "\n",
        "Step 7: Use SGD regressor as an estimator and partial_fit to fit the dataset on the model. (Set random_state=10)\n",
        "\n",
        "Step 8: Calculate different evaluation metrics value like mean_square_error, R2_score.\n",
        "\n",
        "Use the training set for fitting the model and use the test data to make the predictions.\n",
        "\n",
        "Note: No need to scale the data. It's already scaled.\n",
        "\n",
        "Answer the below questions."
      ],
      "metadata": {
        "id": "GZxxhIzKxDcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tables.flavor import toarray\n",
        "df = pd.read_csv('/content/sample_data/data_for_large_scale.csv')\n",
        "# df = pd.read_csv('https://drive.google.com/file/d/1v-uxWEgTI0GDCOTZOX3shUMkTf1a_CL7/view?usp=sharing')\n",
        "X, y = df[df.columns[:-1]], df[df.columns[-1]]\n",
        "X_array, y_array = X.to_numpy(), y.to_numpy()\n",
        "print(X_array.shape, y_array.shape)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_array, y_array, test_size=0.3, random_state=10)\n",
        "print(X_train.shape)\n",
        "X_train, X_test, y_train, y_test = X_train.reshape(-1,90,10), X_test.reshape(-1,90,10), y_train.reshape(-1,90,1), y_test.reshape(-1,90,1)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9ALROxPxLJx",
        "outputId": "a0182c63-0193-4cb6-eb58-99ba97fb83b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(90000, 10) (90000,)\n",
            "(63000, 10)\n",
            "(700, 90, 10) (700, 90, 1) (300, 90, 10) (300, 90, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "sgd = SGDRegressor(random_state=10)\n",
        "coef = []\n",
        "for iter in range(X_train.shape[0]):\n",
        "  # Need to pass the classes in the first iteration, since it's not guaranteed to have the same classes in all chunks of data.\n",
        "  if iter == 0:\n",
        "    # sgd.partial_fit(X_train[iter, :, :], y_train[iter, :, :], classes=np.unique(y))\n",
        "    sgd.partial_fit(X_train[iter, :, :], y_train[iter, :, :])\n",
        "  else:\n",
        "    sgd.partial_fit(X_train[iter, :, :], y_train[iter, :, :])\n",
        "  coef.append(sgd.coef_)\n",
        "\n",
        "  # print(f'After iteration #{iter}, coef_ is {sgd.coef_}, intercept+ is {sgd.intercept_}')\n",
        "\n",
        "print(sgd.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op86ZXxd1i3Q",
        "outputId": "2fbb0586-d5f2-46ae-c251-56c10f238178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.0054532]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3) What is the value of cofficient corresponding to \"feature-3\"you got after trainig the model using SGDRegressor? (select the closest answer)"
      ],
      "metadata": {
        "id": "8OOavBQe268H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sgd.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLxeukmk26Yv",
        "outputId": "b197c3bc-f883-4efc-9e0c-59811f38ba4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[51.3212522  22.26535893 81.23762868 53.19615839 76.46565732 71.47204651\n",
            " 93.45092409 51.92198178 30.03910055 40.95656774]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4) What is the value of R2 score for test data"
      ],
      "metadata": {
        "id": "sFvjH16G3PpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "y_pred = sgd.predict(X_test.reshape(-1,10))\n",
        "score = r2_score(y_test.reshape(-1,1), y_pred)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeqOWs1N3S1p",
        "outputId": "7c4ab595-fe58-4f3f-ba69-59d7f5571119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9999919793766233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5) What is the value of cofficient corresponding to \"feature-5\" after 5th iteration."
      ],
      "metadata": {
        "id": "Uoi8_7tm39bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coef[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6Ov-ST739Jp",
        "outputId": "5c8734bb-d4bc-4f91-80b4-44a0079d42cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([51.3212522 , 22.26535893, 81.23762868, 53.19615839, 76.46565732,\n",
              "       71.47204651, 93.45092409, 51.92198178, 30.03910055, 40.95656774])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 6-8)\n",
        "\n",
        "This dataset was constructed by adding elevation information to a 2D road network in North Jutland, Denmark (covering a region of 185 x 135 km^2). Elevation values where extracted from a publicly available massive Laser Scan Point Cloud for Denmark. This 3D road network was eventually used for benchmarking various fuel and CO2 estimation algorithms. This dataset can be used by any applications that require to know very accurate elevation information of a road network to perform more accurate routing for eco-routing, cyclist routes etc. For the data mining and machine learning community, this dataset can be used as 'ground-truth' validation in spatial mining techniques and satellite image processing. It has no class labels,\n",
        "\n",
        "Use this dataset to guess some missing elevation information for some points on the road.\n",
        "\n",
        "Column names:\n",
        "\n",
        "OSM_ID: OpenStreetMap ID for each road segment or edge in the graph.\n",
        "\n",
        "LONGITUDE: Web Mercaptor (Google format) longitude\n",
        "\n",
        "LATITUDE: Web Mercaptor (Google format) latitude\n",
        "\n",
        "ALTITUDE: Height in meters.\n",
        "\n",
        "Load the dataset from link(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00246/3D_spatial_network.txt\"). Set parameter chunk size=20000 and iterator =True in pd.read_csv().\n",
        "\n",
        "NOTE: The above file doesn't have column names\n",
        "\n",
        "Scale your whole dataset first with standard scalar using partial_fit method. Then use SGDRegressor(random state=10) on the dataset and answer the following."
      ],
      "metadata": {
        "id": "Wi_fN5245hd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import StandardScaler\n",
        "# ss = StandardScaler()\n",
        "# sgdreg = SGDRegressor(random_state=10, warm_start=True)\n",
        "# # data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00246/3D_spatial_network.txt',chunksize=20000,iterator=True, header=None, names=['OSM_ID', 'LONGITUDE', 'LATITUDE', 'ALTITUDE'])\n",
        "# iter = 1\n",
        "# coef = []\n",
        "# intercept = []\n",
        "# size = 0\n",
        "# for train_df in pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00246/3D_spatial_network.txt',chunksize=20000,iterator=True, header=None, names=['OSM_ID', 'LONGITUDE', 'LATITUDE', 'ALTITUDE']):\n",
        "#   ss.partial_fit(train_df)\n",
        "#   train_df_scaled = ss.transform(train_df)\n",
        "#   # print(f\"Partial scaled output is: {train_df_scaled}\")\n",
        "  \n",
        "#   X_train_partial = train_df_scaled[:, 0:3]  # since there are 3 features in dataset\n",
        "#   y_train_partial = train_df_scaled[:, 3] # last column is the label\n",
        "#   size += X_train_partial.shape[0]\n",
        "#   sgdreg.partial_fit(X_train_partial, y_train_partial)\n",
        "#   sgdreg.partial_fit(X_train_partial, y_train_partial)\n",
        "#   coef.append(sgdreg.coef_)\n",
        "#   intercept.append(sgdreg.intercept_)\n",
        "#   iter += 1"
      ],
      "metadata": {
        "id": "8YXi413f6BAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss = StandardScaler()\n",
        "sgdreg = SGDRegressor(random_state=10)\n",
        "chunksize=20000\n",
        "intercept = []\n",
        "coef = []\n",
        "iter = 1\n",
        "size = 0\n",
        "\n",
        "for train_df in pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00246/3D_spatial_network.txt', header=None, names=['OSM_ID', 'LONGITUDE', 'LATITUDE', 'ALTITUDE'], chunksize=chunksize, iterator=True):\n",
        "  ss.partial_fit(train_df)\n",
        "\n",
        "for train_df in pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00246/3D_spatial_network.txt', header=None, names=['OSM_ID', 'LONGITUDE', 'LATITUDE', 'ALTITUDE'], chunksize=chunksize, iterator=True):\n",
        "  train_df_scaled = ss.transform(train_df)\n",
        "  \n",
        "  X_train_partial = train_df_scaled[:, 1:3]  # exclude 0 which is ID column. columns 1 and 2 are features\n",
        "  # y_train_partial = train_df_scaled[:, 3] # last column is the label\n",
        "  y_train_partial = train_df[train_df.columns[-1]]\n",
        "  size += X_train_partial.shape[0]\n",
        "  sgdreg.partial_fit(X_train_partial, y_train_partial)\n",
        "  coef.append(sgdreg.coef_)\n",
        "  intercept.append(sgdreg.intercept_)\n",
        "  if(iter==7):\n",
        "    print('After iterations #: ', iter)\n",
        "    print(sgdreg.coef_)\n",
        "    print(sgdreg.intercept_)\n",
        "  iter += 1\n",
        "print(iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjeNSCt6_Beh",
        "outputId": "59be71ff-58f8-4432-e792-596e3e14d1f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After iterations #:  7\n",
            "[ 4.7924744  -5.10625804]\n",
            "[21.39333846]\n",
            "23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6) Check how many total samples are there in the dataset?"
      ],
      "metadata": {
        "id": "j539cmMZB-zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MV6izy3CBEf",
        "outputId": "d10bd61f-fbe7-4d52-edf8-bd3971909c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "434874"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7) What is the value of intercept after 7th iteration. (select the closest option)."
      ],
      "metadata": {
        "id": "aOlwns_29Csa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intercept[:7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDtM43Kr9KCO",
        "outputId": "efa164ad-596b-41f3-e8cc-6073c58e4175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([24.44543684]),\n",
              " array([21.27380942]),\n",
              " array([23.94764565]),\n",
              " array([21.56160507]),\n",
              " array([21.63953089]),\n",
              " array([20.54946551]),\n",
              " array([21.39333846])]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8) What is the value of the coefficient corresponding to the longitude feature after the 7th iteration? (select the closest option)."
      ],
      "metadata": {
        "id": "7cauBLgTCmNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coef[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y74OURgCohn",
        "outputId": "d377b09a-4fa5-4eeb-fa1b-b50bccb88701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([ 3.0938621 , -5.22002704]),\n",
              " array([ 3.0938621 , -5.22002704]),\n",
              " array([ 3.0938621 , -5.22002704]),\n",
              " array([ 3.0938621 , -5.22002704]),\n",
              " array([ 3.0938621 , -5.22002704]),\n",
              " array([ 3.0938621 , -5.22002704]),\n",
              " array([ 3.0938621 , -5.22002704]),\n",
              " array([ 3.0938621 , -5.22002704]),\n",
              " array([ 3.0938621 , -5.22002704]),\n",
              " array([ 3.0938621 , -5.22002704])]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 9 - 11\n",
        "Load Iris datset on Colab and use KNN classifier to build the model Using following steps.\n",
        "* Step 1: Load the dataset and split it using train_test_split by keeping: test_size= 0.2 random_state=10\n",
        "* Step 2: Use Normalizer() as a scaling function to scale the data.\n",
        "* Step 3: Use KNeighborsClassifier(K) as an estimator to predict the output."
      ],
      "metadata": {
        "id": "LT8Im08sDWjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import Normalizer\n",
        "iris_data, iris_label = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.2, random_state=10)\n",
        "pipe_knn = Pipeline([('scaler', Normalizer()),\n",
        "                     ('knn', KNeighborsClassifier(n_neighbors=1))\n",
        "                     ])\n",
        "gcv = GridSearchCV(pipe_knn, param_grid={'knn__n_neighbors': [2,3,4]}, scoring='accuracy', n_jobs=1)\n",
        "gcv.fit(X_train, y_train)\n",
        "gcv.best_estimator_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUSPC0AFDdO5",
        "outputId": "1d668455-a336-4757-df23-5a8b5efbf40f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('scaler', Normalizer()),\n",
              "                ('knn', KNeighborsClassifier(n_neighbors=3))])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gcv.best_score_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yC3wQXeMEuN5",
        "outputId": "cdf7cf22-2087-497a-9cc3-90ae8afb6fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9666666666666668"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1 = f1_score(y_test, gcv.best_estimator_.predict(X_test), average='weighted')\n",
        "print(f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEkxebSaE1Mw",
        "outputId": "5f71a7a7-1437-49d7-c814-1d272edee02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9671111111111111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Graded Assignment - 7 (PART - B)\n",
        "Q1) Write a function compute_GridSearchCV which accepts the Kernel and regularization parameters as inputs and returns the Mean cross-validated score of the best_estimator, denoted with best_score_ of the models with the below-mentioned hyperparameters:\n",
        "Split the Iris dataset into train and test sets with 70:30 ratio\n",
        "Import svm.SVC as 'model'\n",
        "kernels = ['linear' , 'rbf']\n",
        "Regularization = [1,15,25]\n",
        "gamma = 'auto'\n",
        "Cross Validation = 4\n",
        "random_state=0\n",
        "Note: Mark the closest option."
      ],
      "metadata": {
        "id": "oheXoTOwFluB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "iris_data, iris_label = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.3, random_state=0)\n",
        "\n",
        "pipe_svc = Pipeline([('svc', SVC(gamma='auto', random_state=0))\n",
        "                     ])\n",
        "param_grid={'svc__kernel': ['linear', 'rbf'],\n",
        "            'svc__C': [1, 15, 25]            \n",
        "            }\n",
        "gcv = GridSearchCV(pipe_svc, param_grid=param_grid, cv=4, return_train_score=True)\n",
        "gcv.fit(X_train, y_train)\n",
        "gcv.best_estimator_\n",
        "print('Mean of the score of best estimator: ', gcv.best_estimator_.score(X_train, y_train).mean())\n",
        "print('Mean training score: ', gcv.cv_results_['mean_train_score'].mean())\n",
        "print('Mean test score: ', gcv.cv_results_['mean_test_score'].mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLdTBSEMGjvL",
        "outputId": "139cb6fb-8a53-4143-d1f6-1c36d2aede92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of the score of best estimator:  0.9809523809523809\n",
            "Mean training score:  0.9883425294817699\n",
            "Mean test score:  0.9541191832858501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2 and Q3\n",
        "Read the instructions given below to answer the two questions given below.\n",
        "\n",
        "Split the Social_Network_Ads dataset\n",
        "(https://drive.google.com/file/d/1qUa1GlG4X4ZY_4E0e7jPR-z7AG7NIDbE/view?usp=sharing) into training and test set in 75:25 ratio.\n",
        "\n",
        "Fit and transform the train and test set of the feature matrix by applying StandardScaler transformer.\n",
        "\n",
        "Fit a linear SVM (with random_state = 0and linear kernel) to training data."
      ],
      "metadata": {
        "id": "vT4sGcffJjAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "sna_dataset = pd.read_csv('/content/sample_data/Social_Network_Ads.csv')\n",
        "# print(sna_dataset.head())\n",
        "X = sna_dataset.drop(columns='Purchased')\n",
        "y = sna_dataset['Purchased']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "pipe = Pipeline([('scaler', StandardScaler()),\n",
        "                 ('svc', SVC(kernel='linear', random_state=0) )\n",
        "                 ])\n",
        "pipe.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7gbT7a3JrTb",
        "outputId": "9d811045-c498-4547-abff-9823099ebd6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('scaler', StandardScaler()),\n",
              "                ('svc', SVC(kernel='linear', random_state=0))])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2) The predicted data returns an accuracy_score on test data. Which of the following option represents the calculated accuracy_score?"
      ],
      "metadata": {
        "id": "GJhJD_fWLar_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = pipe.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy score: ', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHXvdXjNLaWy",
        "outputId": "0d7d771e-fa44-443c-ef2b-635fa2d59b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score:  0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3) Calculate the confusion matrix obtained from the above-predicted data."
      ],
      "metadata": {
        "id": "kgiQ11ZwNLio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics._plot.confusion_matrix import confusion_matrix\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6ivesF-NM7q",
        "outputId": "495ed3e7-648c-492f-e5ed-332930ff3155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[66  2]\n",
            " [ 8 24]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the MNIST dataset, consider the first 20,000 data samples as training data and the next 5,000 data samples as test data. Fit a pipeline with MinMaxScaler and a classifier with SVC, linear kernel, one vs rest decision_function_shape and class_weight=None to this dataset and answer the following questions(Q.no 4 and Q.no 5)."
      ],
      "metadata": {
        "id": "L9zBCKlFYpTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = X[:20000], X[20000:25000], y[:20000], y[20000:25000]\n",
        "pipe = Pipeline([('scaler', MinMaxScaler()),\n",
        "                 ('svc', SVC(kernel='linear', decision_function_shape='ovr', class_weight=None))\n",
        "                 ])\n",
        "pipe.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "vXHa0irmYqpw",
        "outputId": "beb776cc-4c64-4a0d-9f67-fde879be1af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-61211556d2b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_openml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_openml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist_784'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_X_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m25000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m25000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m pipe = Pipeline([('scaler', MinMaxScaler()),\n\u001b[1;32m      5\u001b[0m                  \u001b[0;34m(\u001b[0m\u001b[0;34m'svc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision_function_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ovr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36mfetch_openml\u001b[0;34m(name, version, data_id, data_home, target_column, cache, return_X_y, as_frame)\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0mtarget_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0mdata_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0mmd5_checksum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_description\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"md5_checksum\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_download_data_to_bunch\u001b[0;34m(url, sparse, data_home, as_frame, features_list, data_columns, target_columns, shape, md5_checksum)\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0mencode_nominal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mas_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0mparse_arff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_arff\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0mmd5_checksum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmd5_checksum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m     )\n\u001b[1;32m    663\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnominal_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_load_arff_response\u001b[0;34m(url, data_home, return_type, encode_nominal, parse_arff, md5_checksum)\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mparsed_arff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_arff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m# consume remaining stream, if early exited\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_convert_arff_data_dataframe\u001b[0;34m(arff, columns, features_dict)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns_to_keep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_chunk_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marff_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns_to_keep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_chunk_generator\u001b[0;34m(gen, chunksize)\u001b[0m\n\u001b[1;32m    703\u001b[0m     chunk may have a length less than ``chunksize``.\"\"\"\n\u001b[1;32m    704\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/externals/_arff.py\u001b[0m in \u001b[0;36mdecode_rows\u001b[0;34m(self, stream, conversors)\u001b[0m\n\u001b[1;32m    472\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mBadDataFormat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/externals/_arff.py\u001b[0m in \u001b[0;36m_decode_values\u001b[0;34m(values, conversors)\u001b[0m\n\u001b[1;32m    479\u001b[0m             values = [None if value is None else conversor(value)\n\u001b[1;32m    480\u001b[0m                       \u001b[0;32mfor\u001b[0m \u001b[0mconversor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m                       in zip(conversors, values)]\n\u001b[0m\u001b[1;32m    482\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'float: '\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/externals/_arff.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             values = [None if value is None else conversor(value)\n\u001b[0;32m--> 480\u001b[0;31m                       \u001b[0;32mfor\u001b[0m \u001b[0mconversor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                       in zip(conversors, values)]\n\u001b[1;32m    482\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = pipe.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "YDTBPtSCaIx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4) What is the sum of the main diagonal elements of the confusion matrix?"
      ],
      "metadata": {
        "id": "pYqG7yMjZVOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm.diagonal().sum()"
      ],
      "metadata": {
        "id": "P463ashqaeog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5) Which of the following are the correct values of precision, recall and f1_Score?"
      ],
      "metadata": {
        "id": "IZBMXsEhbCIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "1FJOPOkSbEs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6) Consider the MNIST dataset, split it into training and test set in 50:50 ratio with random_state = 42. \n",
        "* Fit a SVM model using pipeline with StandardScalar, SVM classifier kernel='poly' and degree = 3, decision_function_shape='ovr'and class_weight='balanced', C=10. \n",
        "* Train the model on training data, and make predictions for test data.\n",
        "* Generate the Classification report and choose the correct value for weighted avg of f1_score."
      ],
      "metadata": {
        "id": "uwzcIOHFbpts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.datasets import fetch_openml\n",
        "# X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "# pipe = Pipeline([('scaler', StandardScaler()),\n",
        "#                  ('svc', SVC(kernel='poly', degree=3, decision_function_shape='ovr', class_weight='balanced', C=10))\n",
        "#                  ])\n",
        "# pipe.fit(X_train, y_train)\n",
        "# y_pred = pipe.predict(X_test)\n",
        "# print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "dhfAf6dmbrbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7) Write the function compute_score(X_train, y_train, X_test, y_test) to do the following on the Iris dataset-\n",
        "\n",
        "Write your code keeping in mind:\n",
        "* Split the Iris dataset into train and test set with 70:30 ratio\n",
        "* Import svm.SVC as 'model' kernel as 'poly', regularization parameter as 10 and gamma as 'auto'\n",
        "* Train the 'model' and mark the computed 'score'\n",
        "* Train the model and mark the computed accuracy score for test data."
      ],
      "metadata": {
        "id": "656GLcMHc1fU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "svc = SVC(kernel='poly', C=10, gamma='auto')\n",
        "svc.fit(X_test, y_test)\n",
        "train_score = svc.score(X_train, y_train)\n",
        "test_score = svc.score(X_test, y_test)\n",
        "print('Train score: ', train_score)\n",
        "print('Tesrt score: ', test_score)\n",
        "print('Accuracy score: ', accuracy_score(y_test, svc.predict(X_test)))"
      ],
      "metadata": {
        "id": "Iz12WdrkdGdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8) Write the function compute_score(X_train, y_train, X_test, y_test) to do the following on the Iris dataset-\n",
        "\n",
        "Write your code keeping in mind the following:\n",
        "* Split the Iris dataset into train and test sets with 70:30 ratio\n",
        "* Import svm.SVC as 'model', kernel as 'sigmoid', regularization parameter as 25, and gamma as 'auto'\n",
        "* Train the 'model' and mark the computed 'accuracy score' for the test data.'"
      ],
      "metadata": {
        "id": "9gyNCCXleiVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "svc = SVC(kernel='sigmoid', C=25, gamma='auto')\n",
        "svc.fit(X_train, y_train)\n",
        "train_score = svc.score(X_train, y_train)\n",
        "test_score = svc.score(X_test, y_test)\n",
        "print('Train score: ', train_score)\n",
        "print('Tesrt score: ', test_score)\n",
        "print('Accuracy score: ', accuracy_score(y_test, svc.predict(X_test)))"
      ],
      "metadata": {
        "id": "FfgVG2Xoeubq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9) Import the iris dataset and drop the rows where class=Iris-setosa. Apply a pipeline containing a MinMaxScaler()function calledScaler and a svm.svc() called classifier. Split the iris dataset into 75:25 ratio with random_state=0. Mark the correct precision score."
      ],
      "metadata": {
        "id": "Vbg_bCn-fBXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, classification_report\n",
        "from sklearn.datasets import load_iris\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X, y = X[np.where(y!=0)], y[np.where(y!=0)]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "pipe = Pipeline([('scaler', MinMaxScaler()),\n",
        "                 ('svc', SVC())\n",
        "                 ])\n",
        "pipe.fit(X_train, y_train)\n",
        "y_pred = pipe.predict(X_test)\n",
        "score = precision_score(y_test, y_pred)\n",
        "print('Precision Score: ', score)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "sKDeaa7IfD7n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}